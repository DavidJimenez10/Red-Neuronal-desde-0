{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DavidJimenez10/Red-Neuronal-desde-0/blob/main/RedNeuronalDesdeCero.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IT94haonZGv1"
      },
      "source": [
        "#Red neuronal desde 0\n",
        "Este notebook muestra el proceso de la creacion de una red neuronal densa desde 0, utilizando unicamente numpy y programacion orientada a objetos en python. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cyh_wkQ0aD8X"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "seed = 12345\n",
        "rng = np.random.default_rng(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7i_yZgpty5yX",
        "outputId": "537d9cb7-26b7-4f0c-b3b4-e7803942f60e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting nnfs\n",
            "  Downloading nnfs-0.5.1-py3-none-any.whl (9.1 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from nnfs) (1.21.6)\n",
            "Installing collected packages: nnfs\n",
            "Successfully installed nnfs-0.5.1\n"
          ]
        }
      ],
      "source": [
        "!pip install nnfs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N40z6tqPzDhd"
      },
      "outputs": [],
      "source": [
        "import nnfs\n",
        "from nnfs.datasets import spiral_data\n",
        "nnfs.init()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvjpf3wLZdEH"
      },
      "source": [
        "## Capas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FH1zlxqkZiy-"
      },
      "outputs": [],
      "source": [
        "class LayerDense():\n",
        "  \"\"\"Clase para una capa densa\n",
        "  \"\"\"\n",
        "  def __init__(self, n_inputs, n_neurons, weight_regularizer_l1 = 0, weight_regularizer_l2 = 0, bias_regularizer_l1 = 0, bias_regularizer_l2 = 0):\n",
        "    \"\"\"\n",
        "    n_inputs <int> : numero de entradas de la capa\n",
        "    n_neurons <int> : numero de neuronas de la capa\n",
        "    weight_regularizer_l1 <float> : hiperparametro regularizacion l1 (sumatoria valor absoluto pesos)\n",
        "    weight_regularizer_l2 <float> : hiperparametro regularizacion l2 (sumatoria valor cuadrado pesos) \n",
        "    bias_regularizer_l1 <float> : hiperparametro regularizacion l1 (sumatoria valor absoluto sesgo)\n",
        "    bias_regularizer_l2 <float> : hiperparametro regularizacion l2 (sumatoria valor cuadrado sesgo)\n",
        "    \"\"\"\n",
        "    self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
        "    self.biases = np.zeros((1, n_neurons))\n",
        "\n",
        "    self.weight_regularizer_l1 = weight_regularizer_l1\n",
        "    self.bias_regularizer_l1 = bias_regularizer_l1\n",
        "    self.weight_regularizer_l2 = weight_regularizer_l2\n",
        "    self.bias_regularizer_l2 = bias_regularizer_l2\n",
        "\n",
        "  \n",
        "  def forward(self, inputs, training):\n",
        "    \"\"\"Propagacion hacia adelante de la capa\n",
        "    self.output <nparray> : la salida de la red, se calcula facilmente en su forma vectorial XW + B\n",
        "    \"\"\"\n",
        "\n",
        "    self.inputs = inputs\n",
        "    self.output = np.dot(inputs, self.weights) + self.biases\n",
        "\n",
        "  \n",
        "  def backward(self, dvalues):\n",
        "    \"\"\"Propagacion hacia atras de la capa\n",
        "    dvalues <np.array> : vector con el valor de las derivadas de la capa proxima\n",
        "    \"\"\"\n",
        "    #Derivada de los pesos y sesgos\n",
        "    self.dweights = np.dot(self.inputs.T, dvalues)\n",
        "    self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
        "\n",
        "    #L1\n",
        "    #La derivada de L1, al ser una sumatoria de los pesos es -> 1 si w positivo\n",
        "    #                                                          -1 si w es negativo\n",
        "    if self.weight_regularizer_l1 > 0:\n",
        "      dL1 = np.ones_like(self.weights)\n",
        "      dL1[self.weights < 0] = -1\n",
        "      self.dweights += self.weight_regularizer_l1 * dL1 \n",
        "\n",
        "    if self.bias_regularizer_l1 > 0:\n",
        "      dL1 = np.ones_like(self.biases)\n",
        "      dL1[self.biases < 0] = -1\n",
        "      self.dbiases += self.bias_regularizer_l1 * dL1\n",
        "\n",
        "    #L2\n",
        "    #La derivada de L1, al ser una sumatoria de los pesos es -> 2 * w\n",
        "    if self.weight_regularizer_l2 > 0:\n",
        "      self.dweights += 2 * self.weight_regularizer_l2 * self.weights\n",
        "\n",
        "    if self.bias_regularizer_l2 > 0:\n",
        "      self.dbiases += 2 * self.bias_regularizer_l2 * self.biases\n",
        "\n",
        "\n",
        "    self.dinputs = np.dot(dvalues, self.weights.T)\n",
        "\n",
        "class LayerDropout():\n",
        "  \"\"\"Clase para añadir Dropout una capa\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self,dropout_rate):\n",
        "    \"\"\"\n",
        "    dropout_rate <float> : porcentaje de neuronas cuya salida sera 0 durante una propagacion hacia adelante\n",
        "    \"\"\"\n",
        "    self.rate = 1-dropout_rate\n",
        "\n",
        "  def forward(self,inputs,training):\n",
        "    \"\"\"Propagacion hacia adelante\n",
        "    training <bool> : bandera para estado de entrenamiento. Las neuronas seran desactivadas unicamente en la fase de entrenamiento \n",
        "    \"\"\"\n",
        "    self.inputs = inputs\n",
        "\n",
        "    if not training:\n",
        "      self.output = inputs.copy()\n",
        "      return\n",
        "\n",
        "    #Genera un vector de unos con distribucion binomial. donde \n",
        "    self.binary_mask = np.random.binomial(1,self.rate,size=inputs.shape)/self.rate\n",
        "\n",
        "    self.output = inputs * self.binary_mask\n",
        "\n",
        "  def backward(self,dvalues):\n",
        "    #La propagacion hacia atras es sencillamente aplicar la mascara a las derivadas de la capa proxima\n",
        "    self.dinputs = dvalues * self.binary_mask\n",
        "\n",
        "class LayerInput():\n",
        "  \"\"\"Clase de capa Input\n",
        "  Esta capa representa las entradas de la red\n",
        "  \"\"\"\n",
        "  def forward(self,inputs,training):\n",
        "  \n",
        "    self.output = inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Kjx2xNjyqss"
      },
      "source": [
        "## Funciones de Activacion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ASbnvqV07QY"
      },
      "outputs": [],
      "source": [
        "class ActivationReLU():\n",
        "  \"\"\"Clase funcion ReLu\n",
        "  Esta es la funcion mas comun para las capas ocultas de la red\n",
        "  \"\"\"\n",
        "  def forward(self,inputs,training):\n",
        "    \"\"\"Propagacion hacia adelante\n",
        "    La funcion ReLu solo deja pasar valores positivos, los negativos los hace 0\n",
        "    \"\"\"\n",
        "    self.inputs = inputs\n",
        "    self.output = np.maximum(0,inputs)\n",
        "\n",
        "  def backward(self,dvalues):\n",
        "    \"\"\"Propagacion hacia atras\n",
        "    La derivada de la funcion relu es -> 1 valores positivos\n",
        "                                         0 valores negativos\n",
        "    \"\"\"\n",
        "    self.dinputs = dvalues.copy()\n",
        "    self.dinputs[self.inputs <= 0] = 0\n",
        "\n",
        "class ActivationSoftmax():\n",
        "  \"\"\"Funcion de activacion SoftMax\n",
        "  Esta funcion es comun a la salida de la red para problemas de clasificacion. La salida de las neuronas se convierte en una probabilidad \n",
        "  \"\"\"\n",
        "  def forward(self,inputs,training):\n",
        "  \"\"\"el exponencial se utiliza para que todos los valores sean positivos sin perder informacion de los valores negativos\n",
        "  \"\"\"\n",
        "    self.inputs = inputs\n",
        "\n",
        "    exp_values = np.exp(inputs - np.max(inputs, axis = 1, keepdims = True))\n",
        "\n",
        "    probabilities = exp_values / np.sum(exp_values, axis=1, keepdims=True)\n",
        "\n",
        "    self.output = probabilities\n",
        "\n",
        "  def backward(self,dvalues):\n",
        "    \n",
        "    self.dinputs = np.empty_like(dvalues)#La funcion crea un array pero no lo inicializa \n",
        "\n",
        "    for index, (single_output, single_dvalues) in enumerate(zip(self.output,dvalues)):\n",
        "\n",
        "      single_output = single_output.reshape(-1,1)\n",
        "      #calculo de la matriz jacobiana de derivadas parciales\n",
        "      jacobian_matrix = np.diagflat(single_output) - np.dot(single_output, single_output.T)\n",
        "\n",
        "      self.dinputs[index] = np.dot(jacobian_matrix,single_dvalues)\n",
        "\n",
        "\n",
        "  def predictions(self,outputs):\n",
        "    return np.argmax(outputs,axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6NcMbsPhqx1k"
      },
      "source": [
        "## Funciones de perdida"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xs7aP8SjqxW2"
      },
      "outputs": [],
      "source": [
        "class Loss():\n",
        "  \"\"\"Clase base para funciones de perdida\n",
        "  \"\"\"\n",
        "  def calculate(self,output,y, *, include_regularization = False):\n",
        "  \"\"\"\n",
        "  Devuelve la perdida, esta puede ser o no regularizad. Ademas se acumula los valores para cada epoca de entrenamiento\n",
        "  \"\"\"\n",
        "    sample_losses = self.forward(output,y)\n",
        "    data_loss = np.mean(sample_losses)\n",
        "    \n",
        "    self.accumulated_sum += np.sum(sample_losses)\n",
        "    self.accumulated_count += len(sample_losses)\n",
        "\n",
        "    if not include_regularization:\n",
        "      return data_loss\n",
        "\n",
        "    return data_loss, self.regularization_loss()\n",
        "\n",
        "  def regularization_loss(self):\n",
        "  \"\"\"\n",
        "  Devuelve la perdida regularizada del bias y los pesos, bien sea regularizacion de tipo L1 o L2\n",
        "  \"\"\"\n",
        "    regularization_loss = 0\n",
        "\n",
        "    for layer in self.trainable_layers:\n",
        "      #L1\n",
        "      if layer.weight_regularizer_l1 > 0:\n",
        "        regularization_loss += layer.weight_regularizer_l1 * np.sum(np.abs(layer.weights))\n",
        "\n",
        "      if layer.bias_regularizer_l1 > 0:\n",
        "        regularization_loss += layer.bias_regularizer_l1 * np.sum(np.abs(layer.biases))\n",
        "\n",
        "      #L2\n",
        "      if layer.weight_regularizer_l2 > 0:\n",
        "        regularization_loss += layer.weight_regularizer_l2 * np.sum(layer.weights*layer.weights)\n",
        "\n",
        "      if layer.bias_regularizer_l2 > 0:\n",
        "        regularization_loss += layer.bias_regularizer_l2 * np.sum(layer.biases*layer.biases)\n",
        "\n",
        "      return regularization_loss\n",
        "\n",
        "  def remember_trainable_layers(self,trainable_layers):\n",
        "    \"\"\"Guarda las capas \"entrenables\" para cuando hay regularizacion por dropout\n",
        "    \"\"\"\n",
        "    self.trainable_layers = trainable_layers\n",
        "\n",
        "  def calculate_accumulated(self, *, include_regularization = False):\n",
        "  \"\"\"Devuelve la perdida de la epoca, esta puede ser o no regularizada\n",
        "  \"\"\"\n",
        "    data_loss = self.accumulated_sum/self.accumulated_count\n",
        "\n",
        "    if not include_regularization:\n",
        "      return data_loss\n",
        "    \n",
        "    return data_loss, self.regularization_loss()\n",
        "\n",
        "  def new_pass(self):\n",
        "    \"\"\"Reinicia la suma y contador de acumulado\n",
        "    \"\"\"\n",
        "    self.accumulated_sum = 0\n",
        "    self.accumulated_count = 0\n",
        "  \n",
        "\n",
        "\n",
        "class LossCategoricalCrossEntropy(Loss):\n",
        "  \"\"\"Clase de la funcion de perdida Entropia Cruzada para problemas categoricos\n",
        "  \"\"\"\n",
        "  def forward(self,y_pred,y_real):\n",
        "    \"\"\"La funcion devuelve las probabilidades \n",
        "    \"\"\"\n",
        "    samples = len(y_pred)\n",
        "    #Se acota el valor predicho para que no sea 0\n",
        "    y_pred_clipped = np.clip(y_pred,1e-7,1-1e-7)\n",
        "    #Condicional para diferenciar si y_real tiene one-hot encoding\n",
        "    if len(y_real.shape) == 1:\n",
        "      correct_confidence = y_pred_clipped[\n",
        "                                        range(samples),\n",
        "                                        y_real]\n",
        "    elif len(y_real.shape) == 2:\n",
        "      correct_confidence = np.sum(y_pred_clipped*y_real,\n",
        "                                  axis=1)\n",
        "\n",
        "    negative_log_likehoods = np.log(correct_confidence)\n",
        "\n",
        "    return negative_log_likehoods\n",
        "\n",
        "  def backward(self,dvalues,y_true):\n",
        "    \"\"\"\n",
        "    dvalues : valores prediccion\n",
        "    y_true : valores reales\n",
        "    \"\"\"\n",
        "    samples = len(dvalues)\n",
        "    labels = len(dvalues[0])\n",
        "\n",
        "    #One-hot encoding\n",
        "    if len(y_true.shape) == 1:\n",
        "      y_true = np.eye(labels)[y_true]\n",
        "\n",
        "    #calculando el gradiente\n",
        "    self.dinputs = -y_true/dvalues\n",
        "    #normalizar el gradiente, para evitar que una salida con mas neuronas genere una mayor perdida per se\n",
        "    self.dinputs = self.dinputs / samples\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7kKqVWJuzJYT"
      },
      "outputs": [],
      "source": [
        "#Implementando la propagacion hacia atras simplificada cuando utilizo una funcion softmax en la ultima capa y una perdida Categorical Cross Entropy\n",
        "class ActivationSoftmaxLossCategoricalCrossEntropy():\n",
        "\n",
        "  def __init__(self):\n",
        "    self.activation = ActivationSoftmax()\n",
        "    self.loss = LossCategoricalCrossEntropy()\n",
        "\n",
        "  def forward(self,inputs,y_true):\n",
        "    \"\"\"Propagacion hacia adelante consiste en llamar el metodo forward para la funcion softmax y categorical cross entropy\n",
        "    \"\"\"\n",
        "    self.activation.forward(inputs)\n",
        "    self.output = self.activation.output\n",
        "    return self.loss.calculate(self.output,y_true)\n",
        "\n",
        "  def backward(self,dvalues,y_true):\n",
        "    \"\"\"La propagacion hacia atras\n",
        "    \"\"\"\n",
        "    samples = len(dvalues)\n",
        "    #quitando one-hot encoding del label real\n",
        "    if len(y_true.shape) == 2:\n",
        "      y_true = np.argmax(y_true,axis=1)#Obtengo el indice del valor maximo de cada columna\n",
        "    \n",
        "\n",
        "    self.dinputs = dvalues.copy()\n",
        "    #calculo gradiente como el valor de la salida - 1, es decir, el valor del true label\n",
        "    self.dinputs[range(samples), y_true] -= 1\n",
        "    #Normalizacion del gradiente, tal como se hace en el categorical cross entropy\n",
        "    self.dinputs = self.dinputs / samples"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optimizadores"
      ],
      "metadata": {
        "id": "x2gB5rVwpV3f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y0CYNotl-_Bj"
      },
      "outputs": [],
      "source": [
        "class OptimizerSGD():\n",
        "  \"\"\"Clase para el optimizador de gradiente estocastico, con posibilidad de añadir momentum y decremento al learning rate\n",
        "  \"\"\"\n",
        "  def __init__(self,learning_rate=1.0,decay=0., momentum = 0.):\n",
        "  \"\"\"Constructor Optimizador de Gradiente Estocastico\n",
        "  learning_rate <float>: tasa de aprendizaje\n",
        "  decay <float>: constante para el decremento del learning rate\n",
        "  momentum <float>: importancia dada a los pesos y bias de la iteracion anterior\n",
        "  \"\"\"\n",
        "    self.learning_rate = learning_rate\n",
        "    self.current_learning_rate = learning_rate\n",
        "    self.decay = decay\n",
        "    self.iterations = 0\n",
        "    self.momentum = momentum\n",
        "\n",
        "  def pre_update_params(self):\n",
        "  \"\"\"Decrementa la tasa de aprendizaje si aplica\n",
        "  \"\"\"\n",
        "    if self.decay:\n",
        "      self.current_learning_rate = self.learning_rate * (1.0 / (1.0 + self.decay * self.iterations))\n",
        "\n",
        "  def update_params(self,layer):\n",
        "  \"\"\"Actualiza los pesos del modelo\n",
        "  \"\"\"\n",
        "    if self.momentum:\n",
        "      #inicializacion de la matriz de momentos\n",
        "      if not hasattr(layer, 'weight_momentums'):\n",
        "        layer.weight_momentums = np.zeros_like(layer.weights)\n",
        "        layer.bias_momentums = np.zeros_like(layer.biases)\n",
        "        \n",
        "      weight_updates = self.momentum * layer.weight_momentums - self.current_learning_rate * layer.dweights\n",
        "      layer.weight_momentums = weight_updates \n",
        "\n",
        "      bias_updates = self.momentum * layer.bias_momentums - self.current_learning_rate * layer.dbiases\n",
        "      layer.bias_momentums = bias_updates\n",
        "    \n",
        "    else:\n",
        "      weight_updates = - self.current_learning_rate * layer.dweights\n",
        "      bias_updates = - self.current_learning_rate * layer.dbiases\n",
        "\n",
        "\n",
        "    layer.weights += weight_updates\n",
        "    layer.biases += bias_updates\n",
        "  \n",
        "  def post_update_params(self):\n",
        "  \"\"\"Aumenta contador iteraciones para decremento de learning rate\n",
        "  \"\"\"\n",
        "    self.iterations += 1\n",
        "\n",
        "class OptimizerAdaGrad():\n",
        "  \"\"\"Clase para el adaptive gradient\n",
        "  \"\"\"\n",
        "  def __init__(self,learning_rate=1.0,decay=0., epsilon = 1e-7):\n",
        "    \"\"\"Contructor adaptive gradient\n",
        "    El adaptive gradient \"normaliza\" el cambio de en los pesos en busca de utilizar la mayor cantidad de neuronas posibles.\n",
        "    Adicionalmente se sigue teniendo el mecanismo para decremento del learning rate\n",
        "    epsilon <float>: Valor mayor a 0, para evitar la division por 0\n",
        "    \"\"\"\n",
        "    self.learning_rate = learning_rate\n",
        "    self.current_learning_rate = learning_rate\n",
        "    self.decay = decay\n",
        "    self.iterations = 0\n",
        "    self.epsilon = epsilon\n",
        "\n",
        "  def pre_update_params(self):\n",
        "    if self.decay:\n",
        "      self.current_learning_rate = self.learning_rate * (1.0 / (1.0 + self.decay * self.iterations))\n",
        "\n",
        "  def update_params(self,layer):\n",
        "    #inicializacion de la matriz de momentos\n",
        "    if not hasattr(layer, 'weight_cache'):\n",
        "      layer.weight_cache = np.zeros_like(layer.weights)\n",
        "      layer.bias_cache = np.zeros_like(layer.biases)\n",
        "      \n",
        "    layer.weight_cache += layer.dweights**2\n",
        "\n",
        "    layer.bias_cache += layer.dbiases**2\n",
        "    \n",
        "\n",
        "    layer.weights += -self.current_learning_rate * layer.dweights / (np.sqrt(layer.weight_cache)+self.epsilon)\n",
        "    layer.biases += -self.current_learning_rate * layer.dbiases / (np.sqrt(layer.bias_cache)+self.epsilon)\n",
        "  \n",
        "  def post_update_params(self):\n",
        "    self.iterations += 1\n",
        "\n",
        "class OptimizerRMSprop():\n",
        "  \"\"\"Clase del optimizador root mean square propagation\n",
        "  Es una suerte de adagrad con momentum donde la \"Normalizacion\" de los pesos incluye un termino que es la sumatoria en un momento t-1\n",
        "  \"\"\"\n",
        "  def __init__(self,learning_rate=0.001, decay=0., epsilon=1e-7,rho=0.9):\n",
        "    \"\"\"Contructor root mean square propagation\n",
        "    rho <float>: regula el peso que tiene el momemtum o la normalizacion a la hora de dividir los pesos actuales\n",
        "    \"\"\"\n",
        "    self.learning_rate = learning_rate\n",
        "    self.current_learning_rate = learning_rate\n",
        "    self.decay = decay\n",
        "    self.iterations = 0\n",
        "    self.epsilon = epsilon\n",
        "    self.rho = rho\n",
        "  \n",
        "  def pre_update_params(self):\n",
        "    if self.decay:\n",
        "      self.current_learning_rate = self.current_learning_rate * (1. / (1. + self.decay * self.iterations))\n",
        "\n",
        "  def update_params(self,layer):\n",
        "    if not hasattr(layer,'weight_cache'):\n",
        "      layer.weight_cache = np.zeros_like(layer.weights)\n",
        "      layer.bias_cache = np.zeros_like(layer.biases)\n",
        "\n",
        "    layer.weight_cache = self.rho * layer.weight_cache + (1 - self.rho) * layer.dweights**2\n",
        "    layer.bias_cache = self.rho * layer.bias_cache + (1 - self.rho) * layer.dbiases**2\n",
        "\n",
        "    layer.weights += -self.current_learning_rate * layer.dweights / (np.sqrt(layer.weight_cache) + self.epsilon)\n",
        "    layer.biases += -self.current_learning_rate * layer.dbiases / (np.sqrt(layer.bias_cache) + self.epsilon)\n",
        "\n",
        "  def post_update_params(self):\n",
        "    self.iterations += 1\n",
        "\n",
        "\n",
        "class OptimizerAdam():\n",
        "  \"\"\"Clase optimizador adaptive momentum\n",
        "  Este optimizador es muy parecido a root mean square propagation, pero implementa una aceleracion del entrenamiento al inicio para compensar la inicializacion en 0\n",
        "  \"\"\"\n",
        "  def __init__(self, learning_rate=0.001, decay=0., epsilon=1e-7, beta_1=0.9, beta_2=0.999):\n",
        "  \"\"\"\n",
        "  beta_1 <float>: parametro para acelerar entrenamiento afectando el termino del momentum o pesos en t-1\n",
        "  beta_2 <float>: parametro para acelerar entrenamiento afectando el termino del cache o sumatoria de pesos\n",
        "  \"\"\"\n",
        "    self.learning_rate = learning_rate\n",
        "    self.current_learning_rate = learning_rate\n",
        "    self.decay = decay\n",
        "    self.iterations = 0\n",
        "    self.epsilon = epsilon\n",
        "    self.beta_1 = beta_1\n",
        "    self.beta_2 = beta_2\n",
        "  \n",
        "  def pre_update_params(self):\n",
        "    if self.decay:\n",
        "      self.current_learning_rate = self.current_learning_rate * (1. / (1. + self.decay * self.iterations))\n",
        "\n",
        "  def update_params(self,layer):\n",
        "    if not hasattr(layer,'weight_cache'):\n",
        "      layer.weight_momentums = np.zeros_like(layer.weights)\n",
        "      layer.weight_cache = np.zeros_like(layer.weights)\n",
        "\n",
        "      layer.bias_momentums = np.zeros_like(layer.biases)\n",
        "      layer.bias_cache = np.zeros_like(layer.biases)\n",
        "\n",
        "    layer.weight_momentums = self.beta_1 * layer.weight_momentums + (1 - self.beta_1) * layer.dweights\n",
        "    layer.bias_momentums = self.beta_1 * layer.bias_momentums + (1 - self.beta_1) * layer.dbiases\n",
        "    \n",
        "    weight_momentums_corrected = layer.weight_momentums / (1 - self.beta_1 ** (self.iterations + 1))\n",
        "    bias_momentums_corrected = layer.bias_momentums / (1 - self.beta_1 ** (self.iterations + 1))\n",
        "\n",
        "    layer.weight_cache = self.beta_2 * layer.weight_cache + (1 - self.beta_2) * layer.dweights**2\n",
        "    layer.bias_cache = self.beta_2 * layer.bias_cache + (1 - self.beta_2) * layer.dbiases**2\n",
        "\n",
        "    weight_cache_corrected = layer.weight_cache / (1 - self.beta_2 ** (self.iterations + 1))\n",
        "    bias_cache_corrected = layer.bias_cache / (1 - self.beta_2 ** (self.iterations + 1))\n",
        "\n",
        "    layer.weights += -self.current_learning_rate * weight_momentums_corrected / (np.sqrt(weight_cache_corrected) + self.epsilon)\n",
        "    layer.biases += -self.current_learning_rate * bias_momentums_corrected / (np.sqrt(bias_cache_corrected) + self.epsilon)\n",
        "\n",
        "  def post_update_params(self):\n",
        "    self.iterations += 1\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Clases metricas"
      ],
      "metadata": {
        "id": "0mm2UWKuprwU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Accuracy():\n",
        "  \"\"\"Clase para la metrica presicion \n",
        "  \"\"\"\n",
        "  def calculate(self,predictions,y):\n",
        "    \"\"\"Devuelve la presicion promedio del entrenamiento. Ademas acumula los valores de cada iteracion necesarios para poder obtener la presicion por epoca\n",
        "    \"\"\"\n",
        "    comparisons = self.compare(predictions,y)\n",
        "    accuracy = np.mean(comparisons)\n",
        "\n",
        "    self.accumulated_sum += np.sum(comparisons)\n",
        "    self.accumulated_count += len(comparisons)\n",
        "\n",
        "    return accuracy\n",
        "\n",
        "  def calculate_accumulated(self):\n",
        "\n",
        "    accuracy = self.accumulated_sum/self.accumulated_count\n",
        "\n",
        "    return accuracy\n",
        "\n",
        "  def new_pass(self):\n",
        "    self.accumulated_sum = 0\n",
        "    self.accumulated_count = 0\n",
        "\n",
        "class AccuracyCategorical(Accuracy):\n",
        "  \"\"\"Clase Presicion Categorica\n",
        "  \"\"\"\n",
        "  def __init__(self,*,binary = False):\n",
        "    self.binary = binary\n",
        "  \n",
        "  def init(self,y):\n",
        "    pass\n",
        "\n",
        "  def compare(self,predictions,y):\n",
        "    \"\"\"Devuelve un vector con los valores conincidente entre la prediccion y el y_real\n",
        "    \"\"\"\n",
        "    if not self.binary and len(y.shape) == 2:\n",
        "      y = np.argmax(y,axis=1)\n",
        "    return predictions == y"
      ],
      "metadata": {
        "id": "52e9DCRxLtoW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Objeto Modelo"
      ],
      "metadata": {
        "id": "cRK3cZoapmWj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Model():\n",
        "  \"\"\"Clase del modelo, sus metodos permiten definir, entrenar y guardar el modelo\n",
        "  \"\"\"\n",
        "  def __init__(self):\n",
        "    self.layers = []\n",
        "    #Este parametro guardara una instancia de la simplificacion de propagacion hacia atras, para softmax y loss categorical cross entropy\n",
        "    self.softmax_classifier_output = None\n",
        "  \n",
        "  def add(self,layer):\n",
        "  \"\"\"Esta funcion agrega una capa al modelo\n",
        "  \"\"\"\n",
        "    self.layers.append(layer)\n",
        "\n",
        "  def set(self, *, loss, optimizer, accuracy):\n",
        "  \"\"\"Esta funcion define la perdida, el optimizador y la presicion a utilizar\n",
        "  \"\"\"\n",
        "    self.loss = loss\n",
        "    self.optimizer = optimizer\n",
        "    self.accuracy = accuracy\n",
        "\n",
        "  def train(self,X,y,*,epochs=1,batch_size=None,print_every=1,validation_data=None):\n",
        "  \"\"\"Clase para entrenar el modelo\n",
        "  \"\"\"\n",
        "    self.accuracy.init(y)\n",
        "    \n",
        "    train_step = 1\n",
        "\n",
        "    #Condicional para guardar datos de validacion\n",
        "    if validation_data is not None:\n",
        "      validation_step = 1\n",
        "\n",
        "      X_val, y_val = validation_data\n",
        "\n",
        "    #Condicional para entrenar con batch\n",
        "    if batch_size is not None:\n",
        "      train_steps = len(X) // batch_size\n",
        "\n",
        "      if train_steps * batch_size < len(X):\n",
        "        train_steps += 1\n",
        "\n",
        "      if validation_data is not None:\n",
        "        validation_steps = len(X_val)//batch_size\n",
        "\n",
        "        if validation_steps * batch_size < len(X_val):\n",
        "          validation_steps += 1\n",
        "    \n",
        "    #Ciclo de epocas de entrenamiento\n",
        "    for epoch in range(1, epochs + 1):\n",
        "      \n",
        "      print(f'epoch: {epoch} ')\n",
        "      #Reinicio de las clases\n",
        "      self.loss.new_pass()\n",
        "      self.accuracy.new_pass()\n",
        "\n",
        "      #Ciclo para recorrer batchs\n",
        "      for step in range(train_steps):\n",
        "        #Definiendo el batch\n",
        "        if batch_size is None:\n",
        "          batch_X = X\n",
        "          batch_y = y\n",
        "        else:\n",
        "          batch_X = X[step*batch_size:(step+1)*batch_size]\n",
        "          batch_y = y[step*batch_size:(step+1)*batch_size]\n",
        "\n",
        "        #Propagacion hacia adelante\n",
        "        output = self.forward(batch_X, training = True)\n",
        "\n",
        "        data_loss, regularization_loss = self.loss.calculate(output,batch_y,include_regularization=True)\n",
        "\n",
        "        loss = data_loss + regularization_loss\n",
        "\n",
        "        predictions = self.output_layer_activation.predictions(output)\n",
        "\n",
        "        accuracy = self.accuracy.calculate(predictions,batch_y)\n",
        "\n",
        "        #Propagacion hacia atras\n",
        "        self.backward(output,batch_y)\n",
        "\n",
        "        self.optimizer.pre_update_params()\n",
        "        for layer in self.trainable_layers:\n",
        "          self.optimizer.update_params(layer)\n",
        "        self.optimizer.post_update_params()\n",
        "\n",
        "        #Imprime metricas batch\n",
        "        if not step % print_every or step == train_steps - 1:\n",
        "          print(f'step: {step}, acc: {accuracy:.3f}, loss: {loss:.3f} (data_loss: {data_loss:.3f}, reg_loss: {regularization_loss:.3f}), lr: {self.optimizer.current_learning_rate}')\n",
        "\n",
        "      #Metricas epoca\n",
        "      epoch_data_loss, epoch_regularization_loss = self.loss.calculate_accumulated(include_regularization = True)\n",
        "      epoch_loss = epoch_data_loss + epoch_regularization_loss\n",
        "      epoch_accuracy = self.accuracy.calculate_accumulated()\n",
        "\n",
        "      print(f'training, acc: {epoch_accuracy:.3f}, loss: {epoch_loss:.3f} (data_loss: {epoch_data_loss:.3f} reg_loss: {epoch_regularization_loss:.3f}), lr:{self.optimizer.current_learning_rate}')\n",
        "\n",
        "      #Validacion datos\n",
        "      if validation_data is not None:\n",
        "        self.loss.new_pass()\n",
        "        self.accuracy.new_pass()\n",
        "        #Propagacion hacia adelante por batch\n",
        "        for step in range(validation_steps):\n",
        "          \n",
        "          if batch_size is None:\n",
        "            batch_X = X_val\n",
        "            batch_y = y_val\n",
        "\n",
        "          else:\n",
        "            batch_X = X_val[step*batch_size:(step+1)*batch_size]\n",
        "            batch_y = y_val[step*batch_size:(step+1)*batch_size]\n",
        "\n",
        "          output = self.forward(batch_X, training = False)\n",
        "\n",
        "          self.loss.calculate(output, batch_y)\n",
        "\n",
        "          predictions = self.output_layer_activation.predictions(output)\n",
        "          self.accuracy.calculate(predictions, batch_y)\n",
        "\n",
        "        #Metricas datos validacion\n",
        "        validation_loss = self.loss.calculate_accumulated()\n",
        "        validation_accuracy = self.accuracy.calculate_accumulated()\n",
        "\n",
        "        print(f'validation, acc: {validation_accuracy:.3f}, loss: {validation_loss:.3f}')\n",
        "\n",
        "\n",
        "  def finalize(self):\n",
        "  \"\"\"Esta funcion asigna a todas las capas su respectiva capa previa y proxima\n",
        "  \"\"\"\n",
        "    self.input_layer = LayerInput()\n",
        "\n",
        "    layer_count = len(self.layers)\n",
        "\n",
        "    self.trainable_layers = []\n",
        "\n",
        "    for i in range(layer_count):\n",
        "      #La capa previa de la primer capa seran los datos de entrada, guardados en la clase input_layer\n",
        "      if i == 0:\n",
        "        self.layers[i].prev = self.input_layer\n",
        "        self.layers[i].next = self.layers[i+1]\n",
        "      \n",
        "      elif i < layer_count - 1:\n",
        "        self.layers[i].prev = self.layers[i-1]\n",
        "        self.layers[i].next = self.layers[i+1]\n",
        "      #La ultima capa tendra como capa proxima la clase definida para el calculo de la perdida\n",
        "      else:\n",
        "        self.layers[i].prev = self.layers[i-1]\n",
        "        self.layers[i].next = self.loss\n",
        "        self.output_layer_activation = self.layers[i]\n",
        "      \n",
        "      #Dado que algunas capas pueden hacer referencia a las funciones de activacion, se guardan las capas que si se pueden entrenar\n",
        "      if hasattr(self.layers[i],'weights'):\n",
        "        self.trainable_layers.append(self.layers[i])\n",
        "\n",
        "    self.loss.remember_trainable_layers(\n",
        "        self.trainable_layers\n",
        "    )\n",
        "\n",
        "    #Verifica si el problema es un clasificacion que hace uso de Softmaz y Loss Categorical Cross Entropy\n",
        "    if isinstance(self.layers[-1],ActivationSoftmax) and isinstance(self.loss, LossCategoricalCrossEntropy):\n",
        "      self.softmax_classifier_output = ActivationSoftmaxLossCategoricalCrossEntropy()\n",
        "\n",
        "\n",
        "  def forward(self,X,training):\n",
        "  \"\"\"Aplica la propagacion hacia adelante de todas las capas definidas\n",
        "  \"\"\"\n",
        "    self.input_layer.forward(X,training)\n",
        "\n",
        "    for layer in self.layers:\n",
        "      layer.forward(layer.prev.output, training)\n",
        "\n",
        "    return layer.output\n",
        "\n",
        "  def backward(self,output,y):\n",
        "  \"\"\"Aplica la propagacion hacia atras de todas las capas definidas\n",
        "  \"\"\"\n",
        "  #Si el problema es un clasificacion que hace uso de Softmaz y Loss Categorical Cross Entropy\n",
        "  #Se aplica la propagacion hacia atras simplificada y la salida de esta funcion se pasa como valor de las derivadas de la ultima capa\n",
        "    if self.softmax_classifier_output is not None:\n",
        "      self.softmax_classifier_output.backward(output,y)\n",
        "\n",
        "      self.layers[-1].dinputs = self.softmax_classifier_output.dinputs\n",
        "\n",
        "      for layer in reversed(self.layers[:-1]):\n",
        "        layer.backward(layer.next.dinputs)\n",
        "\n",
        "      return\n",
        "\n",
        "    self.loss.backward(output,y)\n",
        "\n",
        "    for layer in reversed(self.layers):\n",
        "      layer.backward(layer.next.dinputs)"
      ],
      "metadata": {
        "id": "xVichZgnOLlr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qOd5EtJ58zHS"
      },
      "source": [
        "## Entrenando Fashion MNIST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rzWCrqBJ9aEA"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import urllib\n",
        "import urllib.request\n",
        "\n",
        "from zipfile import ZipFile\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9kJqXIOGCdTH",
        "outputId": "cef81527-a8d4-4b38-bee6-bdad549457a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Descargando https://nnfs.io/datasets/fashion_mnist_images.zip y guardando fashion_mnist_images.zip...\n",
            "Archivo descomprimido exitosamente\n"
          ]
        }
      ],
      "source": [
        "URL = \"https://nnfs.io/datasets/fashion_mnist_images.zip\"\n",
        "FILE = \"fashion_mnist_images.zip\"\n",
        "FOLDER = \"fashion_mnist_images\"\n",
        "\n",
        "if not os.path.isfile(FILE):\n",
        "  print(f'Descargando {URL} y guardando {FILE}...')\n",
        "  urllib.request.urlretrieve(URL,FILE)\n",
        "\n",
        "with ZipFile(FILE) as zip_images:\n",
        "  zip_images.extractall(FOLDER)\n",
        "\n",
        "print('Archivo descomprimido exitosamente')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YShFqWLb-a2w",
        "outputId": "d2e6f176-7dc0-41a5-e8d5-f93cc4841c54"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[  0   0   0   0   0   0   0   0   0   0  90 156 177 182 196 176 117   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   2   0   0 239 253 239 214 226 214 231 245 248   0   0   1   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0  36 241 203 219 192 190 193 207 200 232 135   0   0   2   0   0   0   0   0   0]\n",
            " [  0   0   0   0   1   0   0  33 167 227 229 234 228 234 244 215 211 214 208 120   0   0   2   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0 157 227 207 207 229 229 232 207 241 227 235 224 203 221 176   0   0   2   0   0   0   0]\n",
            " [  0   0   0   0   0  65 218 189 192 187 196 189 188 202 181 195 222 219 190 180 217 125   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0 158 213 196 196 191 188 186 179 192 177 192 126 155 193 189 200 166   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0 163 217 211 176 188 188 186 183 196 180 191 165 170 216 204 200 195   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0 183 215 234 185 182 186 183 184 198 181 185 195 188 203 245 207 209  37   0   0   0   0   0]\n",
            " [  0   0   0   0   0 196 215 240 209 188 183 186 186 190 186 188 188 183 212 253 205 217  64   0   0   0   0   0]\n",
            " [  0   0   0   0   0 235 213 246 209 190 190 195 194 192 192 192 196 189 209 252 206 217 109   0   0   0   0   0]\n",
            " [  0   0   0   0  18 246 209 253 212 201 209 195 202 204 203 206 207 201 214 251 205 216 160   0   0   0   0   0]\n",
            " [  0   0   0   0  48 249 208 253 217 196 197 191 195 193 194 196 196 194 214 247 206 216 189   0   0   0   0   0]\n",
            " [  0   0   0   0  71 244 206 255 216 195 196 194 197 196 196 195 203 197 197 238 216 215 184   0   0   0   0   0]\n",
            " [  0   0   0   0  91 248 206 255 211 194 197 193 196 195 195 195 199 201 191 247 217 214 198   0   0   0   0   0]\n",
            " [  0   0   0   0 109 224 206 237 187 195 198 193 197 195 194 194 197 203 197 193 211 217 215   0   0   0   0   0]\n",
            " [  0   0   0   0 118 230 205 222 166 204 195 195 198 196 193 192 196 200 206 161 207 219 220   0   0   0   0   0]\n",
            " [  0   0   0   0 110 234 210 183 147 217 189 193 197 197 194 191 194 198 212 136 213 221 220   5   0   0   0   0]\n",
            " [  0   0   0   0 109 239 209 167 146 219 186 192 196 198 194 191 192 195 223 117 206 227 219  31   0   0   0   0]\n",
            " [  0   0   0   0 105 241 212 142 146 218 182 191 193 196 196 192 190 190 232 100 166 238 218  48   0   0   0   0]\n",
            " [  0   0   0   0  99 240 219 115 152 212 180 190 191 192 198 195 187 185 235  99 133 244 216  63   0   0   0   0]\n",
            " [  0   0   0   0  95 238 225  86 163 203 181 188 190 191 195 195 189 181 226 112 133 245 216  83   0   0   0   0]\n",
            " [  0   0   0   0 102 238 228  79 165 198 182 186 189 192 192 191 191 182 217 113 131 250 209  95   0   0   0   0]\n",
            " [  0   0   0   0 113 234 239 103 155 199 182 187 191 192 191 190 191 183 212 130 128 243 203 103   0   0   0   0]\n",
            " [  0   0   0   0 114 220 251 106 144 187 180 184 187 188 187 186 185 183 190 144 103 236 202 110   0   0   0   0]\n",
            " [  0   0   0   0 114 208 252  71 176 202 195 198 202 202 201 201 201 197 202 178  81 233 205 116   0   0   0   0]\n",
            " [  0   0   0   0 109 238 255  38 105 166 156 159 161 162 159 156 156 155 170 112  44 241 224 136   0   0   0   0]\n",
            " [  0   0   0   0  33 177 179  16   0   0   0   0   0   0   0   0   0   0   0   0   0 163 165  53   0   0   0   0]]\n"
          ]
        }
      ],
      "source": [
        "image_data = cv2.imread('fashion_mnist_images/train/4/0002.png',cv2.IMREAD_UNCHANGED)\n",
        "np.set_printoptions(linewidth=200)\n",
        "print(image_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "7svXNYciC7Vk",
        "outputId": "83ac7fe2-9289-46c5-f193-18768ac134dd"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "plt.imshow(image_data,cmap='gray')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NKuRZHJSDvIK"
      },
      "outputs": [],
      "source": [
        "def load_mnist_dataset(dataset,path):\n",
        "\n",
        "  labels = os.listdir(os.path.join(path,dataset))\n",
        "\n",
        "  X = []\n",
        "  y = []\n",
        "\n",
        "  for label in labels:\n",
        "    for image in os.listdir(os.path.join(path,dataset,label)):\n",
        "      image_data = cv2.imread(os.path.join(path,dataset,label,image),cv2.IMREAD_UNCHANGED)\n",
        "\n",
        "      X.append(image_data)\n",
        "      y.append(label)\n",
        "\n",
        "  return np.array(X), np.array(y).astype('uint8')\n",
        "\n",
        "def create_data_mnist(path):\n",
        "  X,y = load_mnist_dataset('train',path)\n",
        "  X_test,y_test = load_mnist_dataset('test',path)\n",
        "\n",
        "  return X,y,X_test,y_test\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kNyTQwGcG9KP"
      },
      "outputs": [],
      "source": [
        "X,y,X_test,y_test = create_data_mnist('fashion_mnist_images')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "STuo9T_eHE4B"
      },
      "outputs": [],
      "source": [
        "#Aleatorizando el orden de los datos\n",
        "keys = np.array(range(X.shape[0]))\n",
        "np.random.shuffle(keys)\n",
        "X=X[keys]\n",
        "y=y[keys]\n",
        "#Escalado de las entradas\n",
        "X = (X.astype('float32') - 127.5) / 127.5\n",
        "X_test = (X_test.astype('float32') - 127.5) / 127.5\n",
        "#Flatten de las entradas\n",
        "X = X.reshape(X.shape[0],-1)\n",
        "X_test = X_test.reshape(X_test.shape[0],-1)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(X[100].reshape(28,28),cmap='gray')\n",
        "print(y[100])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        },
        "id": "qpESkUk4BpcA",
        "outputId": "330d1a7d-5213-4dd9-ea5d-972d55ee507f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "4\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPsAAAD4CAYAAAAq5pAIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAQ8UlEQVR4nO3dW4xd1X3H8d+fsfEVfMFgjzE4iYWEUKUaZEy5qFDiIoIQJkZC8UNlJFTnIUaJlIcCfQgvFahqkuahijQp2E5JCZYSLuYShSIkGh4Cg+WAL1Aba6zYjG0CNoyvYw//PswmmsDs/zqcvc/Fs74fyZqZ8589Z3njH/vM+e+1lrm7AEx853R6AADag7ADmSDsQCYIO5AJwg5kYlI7n8zMeOu/BaZOnVpaO+ec+P/ns2fPDus9PT1h/eOPPw7rIyMjpbXTp0+Hxw4PD4d1jM/dbbzHK4XdzG6V9BNJPZL+090fqfLz0JwlS5aU1qZNmxYee8cdd4T1OXPmhPXnn38+rB8+fLi0dvDgwfDYgYGBsJ5iNu6/eUlSji3npl/Gm1mPpP+Q9A1JV0habWZX1DUwAPWq8jv7ckm73X2Puw9L+qWklfUMC0DdqoT9Ykl/HPP1vuKxv2Bma82s38z6KzwXgIpa/gadu/dJ6pN4gw7opCpX9v2SLhnz9aLiMQBdqErY35B0mZl91czOlfQtSc/WMywAdbMqLQgzu03Sv2u09faYu/9L4vvP2pfxVdo4vb29YX3FihVh/Z577gnrN998c2lt586d4bGTJsW/yaX67FGPX5Lef//90tp7770XHvvcc8+F9ccffzys56olfXZ3f0HSC1V+BoD24HZZIBOEHcgEYQcyQdiBTBB2IBOEHchEW+ezn82q3I8wZcqUsH7llVeG9Q0bNoT1V155pbR21113hcem5rufOXMmrE+ePDms7969u7T22muvhccuXbo0rFfps0f3TUgTcwosV3YgE4QdyARhBzJB2IFMEHYgE4QdyASttzZIraJ67NixsL5r166wfu2115bWPv300/DYEydOhPVUiyq1lHT0d9+yZUt47Ouvvx7WUyvfRivbTsTWWgpXdiAThB3IBGEHMkHYgUwQdiAThB3IBGEHMlFpKekv/WRn8VLSVWzatCmsX3311WE9Wo5Zkq677rrS2t69e8NjU/XUFNhoB1lJWrBgQWlt8+bN4bGpPvxVV10V1leuzHPrwbKlpLmyA5kg7EAmCDuQCcIOZIKwA5kg7EAmCDuQCfrsNbjvvvvC+rp168L60NBQWJ82bVpYj+asp/77Dg8Ph/WUmTNnhvXo75ba7vn48eNhPbVE96pVq0pre/bsCY89m7Vky2YzG5A0JGlE0hl3X1bl5wFonTpWqvk7d/9TDT8HQAvxOzuQiaphd0m/NbM3zWzteN9gZmvNrN/M+is+F4AKqr6Mv8Hd95vZRZJeMrN33P3Vsd/g7n2S+qSJ+wYdcDaodGV39/3Fx0OSnpK0vI5BAahf02E3sxlmdt5nn0u6RdK2ugYGoF5VXsbPl/RUsa74JEn/7e6/qWVUZ5kVK1aE9dTa66m121N6enpKa6leddUtm0+dOhXWo3sAPvjgg/DYc889N6yn7j+48cYbS2sTuc9epumwu/seSX9d41gAtBCtNyAThB3IBGEHMkHYgUwQdiATbNlcg9TWwUePHg3rqRZTalvkqH02MjJS6blTrbuo7SfFU2BTrbPUz05N3122rHwS5vr168NjJyKu7EAmCDuQCcIOZIKwA5kg7EAmCDuQCcIOZII+ew0uvPDCsJ7qk1900UVhPdVPPnDgQGmtaq86JbUUdbTd9PTp08NjU+c1tZT0woULw3puuLIDmSDsQCYIO5AJwg5kgrADmSDsQCYIO5AJ+uw1SPV7o+WUpfRS0qktnefOndv0sadPnw7rqaWmU/Plo156qsdf9bz19vaG9dxwZQcyQdiBTBB2IBOEHcgEYQcyQdiBTBB2IBP02Rt03nnnldZS/eLU2uuzZs0K61OnTg3rUT861QdP/exUHz61HXU0nz71s1P3L6TuAUj16XOTvLKb2WNmdsjMto15bK6ZvWRmu4qP8S4JADqukZfxGyTd+rnH7pf0srtfJunl4msAXSwZdnd/VdJHn3t4paSNxecbJd1Z87gA1KzZ39nnu/tg8fkBSfPLvtHM1kpa2+TzAKhJ5Tfo3N3NrPQdKnfvk9QnSdH3AWitZltvB82sV5KKj4fqGxKAVmg27M9KWlN8vkbSM/UMB0CrJF/Gm9kTkm6SNM/M9kn6gaRHJG0ys3sl7ZV0dysH2Q2iOeOpPc5Ta6tHe5hL0pEjR8L6yZMnS2upewBSve5Urzr186M556k++aRJ8T/P1NgnT55cWkvtDZ+aK382Sobd3VeXlL5e81gAtBC3ywKZIOxAJgg7kAnCDmSCsAOZYIprg6Ltg1Ptp6gFJEmzZ88O69u3bw/rUYsq9bNT029TUlNco7bkmTNnwmNTbb9U6y36bzZ/fukd3pKkgYGBsH424soOZIKwA5kg7EAmCDuQCcIOZIKwA5kg7EAm6LM3KJqOWXW55VOnToX1aBlrKV6uOTVVM3UPQNUtnaPloFN/79TYUvcIROc9Na14IuLKDmSCsAOZIOxAJgg7kAnCDmSCsAOZIOxAJuiz1yA1n71qnz21pHK0VHXVbYtTc86jHr8kDQ4OltZSyzmnzmuqxx+d1xkzZoTHTkRc2YFMEHYgE4QdyARhBzJB2IFMEHYgE4QdyAR99gZFc8pTvejUls6p+tGjR8P69OnTw3qkah899dyLFi0qrQ0NDYXHpu4RmDp1aliPzuv5558fHjsRJa/sZvaYmR0ys21jHnvIzPab2dbiz22tHSaAqhp5Gb9B0q3jPP5jd19a/Hmh3mEBqFsy7O7+qqSP2jAWAC1U5Q26dWb2VvEyf07ZN5nZWjPrN7P+Cs8FoKJmw/5TSUskLZU0KOmHZd/o7n3uvszdlzX5XABq0FTY3f2gu4+4+6eSfiZpeb3DAlC3psJuZr1jvvympG1l3wugOyT77Gb2hKSbJM0zs32SfiDpJjNbKsklDUj6dgvH2BVSa5hHUvOyU/3kVP3YsWNNP3eqV33y5Mmwnlq7/ZNPPimtpe4vuOCCC8J66h6BaD576u89ESXD7u6rx3n40RaMBUALcbsskAnCDmSCsAOZIOxAJgg7kAmmuDYoar1V3bI5NdXz8OHDYT1aFnnBggXhsanWWqr9deTIkbAetQWjmpSePjtnTuld2pLiliVbNgOYsAg7kAnCDmSCsAOZIOxAJgg7kAnCDmSCPnuDqmzxm5pOmZqquXjx4rAe3QNw4sSJ8NhUPbVd9Lx588L6rFmzSmtVp/amRNN7U0tkT0Rc2YFMEHYgE4QdyARhBzJB2IFMEHYgE4QdyAR99gZFc9JT881Tc6enTZsW1vft2xfWzzmn/P/ZVbcmHh4eDuuppaqj7aZT9y5E22RL6fsXBgcHw3puuLIDmSDsQCYIO5AJwg5kgrADmSDsQCYIO5AJ+uwNivrFqa2HU73q1JzyVD85ev7UXPkqW1FLcY9fkkZGRkprqfOSWm8/9dyLFi0qrfX394fHTkTJK7uZXWJmr5jZDjPbbmbfLR6fa2Yvmdmu4mO8Yj+AjmrkZfwZSd939ysk/Y2k75jZFZLul/Syu18m6eXiawBdKhl2dx909y3F50OSdkq6WNJKSRuLb9so6c5WDRJAdV/qd3Yz+4qkKyX9XtJ8d//s5uMDkuaXHLNW0trmhwigDg2/G29mMyX9StL33P2TsTUfnQ0x7owId+9z92XuvqzSSAFU0lDYzWyyRoP+C3f/dfHwQTPrLeq9kg61ZogA6pB8GW+j/Y9HJe109x+NKT0raY2kR4qPz7RkhF1i165dpbVrrrkmPHbz5s1hPdW6S21dHLW3Uu2r1HbTqeWcU9NUo6WkU1s2V53Cesstt5TWPvzww/DYiaiR39mvl/QPkt42s63FYw9qNOSbzOxeSXsl3d2aIQKoQzLs7v47SWWXh6/XOxwArcLtskAmCDuQCcIOZIKwA5kg7EAmLLUUcK1PZta+J2ujyy+/PKxHvWZJ2rBhQ1g/cuRIWE/10iOpLZmjqb1SuhceSf29Lr300rD+wAMPhPVnnpnQt36Ucvdx/0FwZQcyQdiBTBB2IBOEHcgEYQcyQdiBTBB2IBMsJV2Dd955J6yn+vCpJZFTpkyZUlpL9clT89kXLlwY1oeGhsJ6tJR1akvm1P0DVbejzg1XdiAThB3IBGEHMkHYgUwQdiAThB3IBGEHMkGfvUE9PT2ltWjddindR0+tKdDK+erHjx8P6wcOHAjr0XmR4i2hU+cttV5+qscfSY07NbazEVd2IBOEHcgEYQcyQdiBTBB2IBOEHcgEYQcy0cj+7JdI+rmk+ZJcUp+7/8TMHpL0j5I+KL71QXd/oVUD7bQqfdcdO3aE9RdffDGsr1q1KqxHvfBorrskzZw5M6ynpM5L1M9evHhxeOymTZvC+tNPPx3WI6l95yeiRm6qOSPp++6+xczOk/Smmb1U1H7s7v/WuuEBqEsj+7MPShosPh8ys52SLm71wADU60v9zm5mX5F0paTfFw+tM7O3zOwxM5tTcsxaM+s3s/5KIwVQScNhN7OZkn4l6Xvu/omkn0paImmpRq/8PxzvOHfvc/dl7r6shvECaFJDYTezyRoN+i/c/deS5O4H3X3E3T+V9DNJy1s3TABVJcNuo1OuHpW0091/NObx3jHf9k1J2+ofHoC6JLdsNrMbJP2vpLclfdaveFDSao2+hHdJA5K+XbyZF/2sCbllc6utX78+rC9fXv6iKppiKqWnic6ePTusp6bIRq25J598Mjz24YcfDusYX9mWzY28G/87SeMdPGF76sBExB10QCYIO5AJwg5kgrADmSDsQCYIO5CJZJ+91iejz94S119/fWnt9ttvD4999913w3pqW+TUls/RPQInT54Mj01JLdGd4zRWqbzPzpUdyARhBzJB2IFMEHYgE4QdyARhBzJB2IFMtLvP/oGkvWMemifpT20bwJfTrWPr1nFJjK1ZdY5tsbtfOF6hrWH/wpOb9Xfr2nTdOrZuHZfE2JrVrrHxMh7IBGEHMtHpsPd1+Pkj3Tq2bh2XxNia1ZaxdfR3dgDt0+krO4A2IexAJjoSdjO71czeNbPdZnZ/J8ZQxswGzOxtM9va6f3pij30DpnZtjGPzTWzl8xsV/Fx3D32OjS2h8xsf3HutprZbR0a2yVm9oqZ7TCz7Wb23eLxjp67YFxtOW9t/53dzHok/Z+kv5e0T9Ibkla7e7yJeZuY2YCkZe7e8RswzOxvJR2V9HN3/6visX+V9JG7P1L8j3KOu/9Tl4ztIUlHO72Nd7FbUe/YbcYl3SnpHnXw3AXjulttOG+duLIvl7Tb3fe4+7CkX0pa2YFxdD13f1XSR597eKWkjcXnGzX6j6XtSsbWFdx90N23FJ8PSfpsm/GOnrtgXG3RibBfLOmPY77ep+7a790l/dbM3jSztZ0ezDjmj9lm64Ck+Z0czDiS23i30+e2Ge+ac9fM9udV8QbdF93g7ldJ+oak7xQvV7uSj/4O1k2904a28W6XcbYZ/7NOnrtmtz+vqhNh3y/pkjFfLyoe6wruvr/4eEjSU+q+ragPfraDbvHxUIfH82fdtI33eNuMqwvOXSe3P+9E2N+QdJmZfdXMzpX0LUnPdmAcX2BmM4o3TmRmMyTdou7bivpZSWuKz9dIeqaDY/kL3bKNd9k24+rwuev49ufu3vY/km7T6Dvy70n6506MoWRcX5P0h+LP9k6PTdITGn1Zd1qj723cK+kCSS9L2iXpfyTN7aKx/ZdGt/Z+S6PB6u3Q2G7Q6Ev0tyRtLf7c1ulzF4yrLeeN22WBTPAGHZAJwg5kgrADmSDsQCYIO5AJwg5kgrADmfh/O/CmijR4WD8AAAAASUVORK5CYII=\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Model()\n",
        "model.add(LayerDense(X.shape[1],128))\n",
        "model.add(ActivationReLU())\n",
        "model.add(LayerDense(128,128))\n",
        "model.add(ActivationReLU())\n",
        "model.add(LayerDense(128,10))\n",
        "model.add(ActivationSoftmax())\n",
        "\n",
        "model.set(loss = LossCategoricalCrossEntropy(), optimizer = OptimizerAdam(decay = 5e-7), accuracy = AccuracyCategorical())\n",
        "\n",
        "model.finalize()\n",
        "\n",
        "model.train(X,y,validation_data = (X_test, y_test), epochs=10, batch_size = 128, print_every = 100)"
      ],
      "metadata": {
        "id": "h5hVZtYGEYQv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d17fedc2-8551-4c1b-bd4d-78b8b97dfc75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 1 \n",
            "step: 0, acc: 0.094, loss: -2.303 (data_loss: -2.303, reg_loss: 0.000), lr: 0.001\n",
            "step: 100, acc: 0.758, loss: -0.680 (data_loss: -0.680, reg_loss: 0.000), lr: 0.000997478227317151\n",
            "step: 200, acc: 0.797, loss: -0.653 (data_loss: -0.653, reg_loss: 0.000), lr: 0.00099000066495765\n",
            "step: 300, acc: 0.812, loss: -0.543 (data_loss: -0.543, reg_loss: 0.000), lr: 0.0009776790139111888\n",
            "step: 400, acc: 0.820, loss: -0.521 (data_loss: -0.521, reg_loss: 0.000), lr: 0.000960695936222112\n",
            "step: 468, acc: 0.875, loss: -0.413 (data_loss: -0.413, reg_loss: 0.000), lr: 0.0009466094146227431\n",
            "training, acc: 0.762, loss: -0.645 (data_loss: -0.645 reg_loss: 0.000), lr:0.0009466094146227431\n",
            "validation, acc: 0.826, loss: -0.485\n",
            "epoch: 2 \n",
            "step: 0, acc: 0.797, loss: -0.441 (data_loss: -0.441, reg_loss: 0.000), lr: 0.0009463874867570986\n",
            "step: 100, acc: 0.836, loss: -0.508 (data_loss: -0.508, reg_loss: 0.000), lr: 0.0009221247093119007\n",
            "step: 200, acc: 0.820, loss: -0.516 (data_loss: -0.516, reg_loss: 0.000), lr: 0.0008940040278953239\n",
            "step: 300, acc: 0.812, loss: -0.465 (data_loss: -0.465, reg_loss: 0.000), lr: 0.0008624194562602221\n",
            "step: 400, acc: 0.844, loss: -0.439 (data_loss: -0.439, reg_loss: 0.000), lr: 0.0008278029668835294\n",
            "step: 468, acc: 0.875, loss: -0.345 (data_loss: -0.345, reg_loss: 0.000), lr: 0.0008027658707120492\n",
            "training, acc: 0.848, loss: -0.419 (data_loss: -0.419 reg_loss: 0.000), lr:0.0008027658707120492\n",
            "validation, acc: 0.846, loss: -0.427\n",
            "epoch: 3 \n",
            "step: 0, acc: 0.852, loss: -0.360 (data_loss: -0.360, reg_loss: 0.000), lr: 0.000802389550013093\n",
            "step: 100, acc: 0.852, loss: -0.450 (data_loss: -0.450, reg_loss: 0.000), lr: 0.0007637048814260649\n",
            "step: 200, acc: 0.875, loss: -0.451 (data_loss: -0.451, reg_loss: 0.000), lr: 0.0007232617928787488\n",
            "step: 300, acc: 0.836, loss: -0.382 (data_loss: -0.382, reg_loss: 0.000), lr: 0.0006815461105200836\n",
            "step: 400, acc: 0.844, loss: -0.391 (data_loss: -0.391, reg_loss: 0.000), lr: 0.0006390352796552819\n",
            "step: 468, acc: 0.875, loss: -0.292 (data_loss: -0.292, reg_loss: 0.000), lr: 0.0006099095524236159\n",
            "training, acc: 0.864, loss: -0.372 (data_loss: -0.372 reg_loss: 0.000), lr:0.0006099095524236159\n",
            "validation, acc: 0.856, loss: -0.403\n",
            "epoch: 4 \n",
            "step: 0, acc: 0.867, loss: -0.325 (data_loss: -0.325, reg_loss: 0.000), lr: 0.0006094807826929915\n",
            "step: 100, acc: 0.852, loss: -0.433 (data_loss: -0.433, reg_loss: 0.000), lr: 0.0005666597130049772\n",
            "step: 200, acc: 0.891, loss: -0.408 (data_loss: -0.408, reg_loss: 0.000), lr: 0.000524221490799622\n",
            "step: 300, acc: 0.836, loss: -0.365 (data_loss: -0.365, reg_loss: 0.000), lr: 0.0004825447295158863\n",
            "step: 400, acc: 0.852, loss: -0.361 (data_loss: -0.361, reg_loss: 0.000), lr: 0.0004419678836504839\n",
            "step: 468, acc: 0.875, loss: -0.288 (data_loss: -0.288, reg_loss: 0.000), lr: 0.000415156280200863\n",
            "training, acc: 0.876, loss: -0.341 (data_loss: -0.341 reg_loss: 0.000), lr:0.000415156280200863\n",
            "validation, acc: 0.864, loss: -0.383\n",
            "epoch: 5 \n",
            "step: 0, acc: 0.859, loss: -0.322 (data_loss: -0.322, reg_loss: 0.000), lr: 0.000414767228540492\n",
            "step: 100, acc: 0.852, loss: -0.408 (data_loss: -0.408, reg_loss: 0.000), lr: 0.0003766961298954466\n",
            "step: 200, acc: 0.883, loss: -0.395 (data_loss: -0.395, reg_loss: 0.000), lr: 0.00034041489410722216\n",
            "step: 300, acc: 0.844, loss: -0.340 (data_loss: -0.340, reg_loss: 0.000), lr: 0.00030609534728442885\n",
            "step: 400, acc: 0.859, loss: -0.335 (data_loss: -0.335, reg_loss: 0.000), lr: 0.00027386453232900295\n",
            "step: 468, acc: 0.875, loss: -0.275 (data_loss: -0.275, reg_loss: 0.000), lr: 0.00025318537334636013\n",
            "training, acc: 0.884, loss: -0.319 (data_loss: -0.319 reg_loss: 0.000), lr:0.00025318537334636013\n",
            "validation, acc: 0.869, loss: -0.368\n",
            "epoch: 6 \n",
            "step: 0, acc: 0.875, loss: -0.320 (data_loss: -0.320, reg_loss: 0.000), lr: 0.00025288886115665396\n",
            "step: 100, acc: 0.844, loss: -0.375 (data_loss: -0.375, reg_loss: 0.000), lr: 0.00022435885285625705\n",
            "step: 200, acc: 0.867, loss: -0.385 (data_loss: -0.385, reg_loss: 0.000), lr: 0.00019805595293754662\n",
            "step: 300, acc: 0.859, loss: -0.327 (data_loss: -0.327, reg_loss: 0.000), lr: 0.0001739657997484976\n",
            "step: 400, acc: 0.844, loss: -0.321 (data_loss: -0.321, reg_loss: 0.000), lr: 0.0001520446879177637\n",
            "step: 468, acc: 0.875, loss: -0.269 (data_loss: -0.269, reg_loss: 0.000), lr: 0.00013834313539864255\n",
            "training, acc: 0.889, loss: -0.306 (data_loss: -0.306 reg_loss: 0.000), lr:0.00013834313539864255\n",
            "validation, acc: 0.870, loss: -0.361\n",
            "epoch: 7 \n",
            "step: 0, acc: 0.883, loss: -0.305 (data_loss: -0.305, reg_loss: 0.000), lr: 0.00013814876009319142\n",
            "step: 100, acc: 0.867, loss: -0.344 (data_loss: -0.344, reg_loss: 0.000), lr: 0.00011972633010229828\n",
            "step: 200, acc: 0.891, loss: -0.355 (data_loss: -0.355, reg_loss: 0.000), lr: 0.00010324381336031892\n",
            "step: 300, acc: 0.859, loss: -0.319 (data_loss: -0.319, reg_loss: 0.000), lr: 8.858704173736783e-05\n",
            "step: 400, acc: 0.867, loss: -0.314 (data_loss: -0.314, reg_loss: 0.000), lr: 7.563246675151779e-05\n",
            "step: 468, acc: 0.875, loss: -0.253 (data_loss: -0.253, reg_loss: 0.000), lr: 6.772979930293638e-05\n",
            "training, acc: 0.892, loss: -0.296 (data_loss: -0.296 reg_loss: 0.000), lr:6.772979930293638e-05\n",
            "validation, acc: 0.872, loss: -0.357\n",
            "epoch: 8 \n",
            "step: 0, acc: 0.883, loss: -0.299 (data_loss: -0.299, reg_loss: 0.000), lr: 6.76188030377499e-05\n",
            "step: 100, acc: 0.875, loss: -0.329 (data_loss: -0.329, reg_loss: 0.000), lr: 5.7245546832604546e-05\n",
            "step: 200, acc: 0.898, loss: -0.343 (data_loss: -0.343, reg_loss: 0.000), lr: 4.8222323562829673e-05\n",
            "step: 300, acc: 0.867, loss: -0.316 (data_loss: -0.316, reg_loss: 0.000), lr: 4.0419120607606036e-05\n",
            "step: 400, acc: 0.875, loss: -0.315 (data_loss: -0.315, reg_loss: 0.000), lr: 3.3709941544387404e-05\n",
            "step: 468, acc: 0.865, loss: -0.255 (data_loss: -0.255, reg_loss: 0.000), lr: 2.9710933857969316e-05\n",
            "training, acc: 0.894, loss: -0.291 (data_loss: -0.291 reg_loss: 0.000), lr:2.9710933857969316e-05\n",
            "validation, acc: 0.874, loss: -0.355\n",
            "epoch: 9 \n",
            "step: 0, acc: 0.875, loss: -0.298 (data_loss: -0.298, reg_loss: 0.000), lr: 2.9655300514204666e-05\n",
            "step: 100, acc: 0.875, loss: -0.321 (data_loss: -0.321, reg_loss: 0.000), lr: 2.452508320514988e-05\n",
            "step: 200, acc: 0.898, loss: -0.340 (data_loss: -0.340, reg_loss: 0.000), lr: 2.0181402863756303e-05\n",
            "step: 300, acc: 0.867, loss: -0.319 (data_loss: -0.319, reg_loss: 0.000), lr: 1.6524374342081123e-05\n",
            "step: 400, acc: 0.883, loss: -0.309 (data_loss: -0.309, reg_loss: 0.000), lr: 1.34626828985792e-05\n",
            "step: 468, acc: 0.865, loss: -0.256 (data_loss: -0.256, reg_loss: 0.000), lr: 1.1678264762502848e-05\n",
            "training, acc: 0.896, loss: -0.288 (data_loss: -0.288 reg_loss: 0.000), lr:1.1678264762502848e-05\n",
            "validation, acc: 0.874, loss: -0.354\n",
            "epoch: 10 \n",
            "step: 0, acc: 0.875, loss: -0.298 (data_loss: -0.298, reg_loss: 0.000), lr: 1.1653669692616583e-05\n",
            "step: 100, acc: 0.883, loss: -0.317 (data_loss: -0.317, reg_loss: 0.000), lr: 9.414714926913838e-06\n",
            "step: 200, acc: 0.898, loss: -0.340 (data_loss: -0.340, reg_loss: 0.000), lr: 7.5680650041475245e-06\n",
            "step: 300, acc: 0.867, loss: -0.320 (data_loss: -0.320, reg_loss: 0.000), lr: 6.05335091179965e-06\n",
            "step: 400, acc: 0.883, loss: -0.304 (data_loss: -0.304, reg_loss: 0.000), lr: 4.817705773084764e-06\n",
            "step: 468, acc: 0.865, loss: -0.254 (data_loss: -0.254, reg_loss: 0.000), lr: 4.113173115547511e-06\n",
            "training, acc: 0.896, loss: -0.287 (data_loss: -0.287 reg_loss: 0.000), lr:4.113173115547511e-06\n",
            "validation, acc: 0.874, loss: -0.354\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "qRU4ovBx16j2"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "RedNeuronalDesdeCero.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyM8WCxH3oINeyYoFYs5rpMp",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}