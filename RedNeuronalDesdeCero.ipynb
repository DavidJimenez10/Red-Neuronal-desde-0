{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RedNeuronalDesdeCero.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyN3HLO7xyWacT/tzTg/3rMQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DavidJimenez10/Red-Neuronal-desde-0/blob/main/RedNeuronalDesdeCero.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Red neuronal desde 0\n",
        "Este notebook muestra el proceso de la creacion de una red neuronal densa desde 0, utilizando unicamente numpy y programacion orientada a objetos en python. "
      ],
      "metadata": {
        "id": "IT94haonZGv1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "seed = 12345\n",
        "rng = np.random.default_rng(seed)"
      ],
      "metadata": {
        "id": "cyh_wkQ0aD8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nnfs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7i_yZgpty5yX",
        "outputId": "ac799496-fe91-4a9f-8959-a17eb0142d09"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting nnfs\n",
            "  Downloading nnfs-0.5.1-py3-none-any.whl (9.1 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from nnfs) (1.21.6)\n",
            "Installing collected packages: nnfs\n",
            "Successfully installed nnfs-0.5.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nnfs\n",
        "from nnfs.datasets import spiral_data\n",
        "nnfs.init()"
      ],
      "metadata": {
        "id": "N40z6tqPzDhd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Capas"
      ],
      "metadata": {
        "id": "tvjpf3wLZdEH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerDense():\n",
        "\n",
        "    def __init__(self, n_inputs, n_neurons):\n",
        "        \n",
        "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons)\n",
        "        self.biases = np.zeros((1, n_neurons))\n",
        "\n",
        "    \n",
        "    def forward(self, inputs):\n",
        "        \n",
        "        self.inputs = inputs\n",
        "        \n",
        "        self.output = np.dot(inputs, self.weights) + self.biases\n",
        "\n",
        "    \n",
        "    def backward(self, dvalues):\n",
        "        \n",
        "        self.dweights = np.dot(self.inputs.T, dvalues)\n",
        "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
        "        \n",
        "        self.dinputs = np.dot(dvalues, self.weights.T)"
      ],
      "metadata": {
        "id": "FH1zlxqkZiy-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Funciones de Activacion"
      ],
      "metadata": {
        "id": "_Kjx2xNjyqss"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ActivationReLU():\n",
        "  def forward(self,inputs):\n",
        "    self.inputs = inputs\n",
        "    self.output = np.maximum(0,inputs)\n",
        "\n",
        "  def backward(self,dvalues):\n",
        "    self.dinputs = dvalues.copy()\n",
        "\n",
        "    self.dinputs[self.inputs <= 0] = 0\n",
        "\n",
        "class ActivationSoftmax():\n",
        "  def forward(self,inputs):\n",
        "    inputs_negativos = inputs - np.max(inputs,axis=1,keepdims=True)\n",
        "    exponential = np.exp(inputs_negativos)\n",
        "    sum_batch = np.sum(exponential,axis=1,keepdims=True)\n",
        "    self.output = exponential / sum_batch\n",
        "\n",
        "  def backward(self,dvalues):\n",
        "    #Creo pero no inicializo un array\n",
        "    self.dinputs = np.empty_like(dvalues)\n",
        "\n",
        "    for index, (single_output, single_dvalues) in enumerate(zip(self.output,dvalues)):\n",
        "\n",
        "      single_output = single_output.reshape(-1,1)\n",
        "      #calculo de la matriz jacobiana de derivadas parciales\n",
        "      jacobian_matrix = np.diagflat(single_output) - np.dot(single_output, single_output.T)\n",
        "\n",
        "      self.dinputs[index] = np.dot(jacobian_matrix,single_dvalues)\n",
        "\n"
      ],
      "metadata": {
        "id": "_ASbnvqV07QY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Perdida"
      ],
      "metadata": {
        "id": "6NcMbsPhqx1k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Loss():\n",
        "  def calculate(self,output,y):\n",
        "    sample_loss = self.forward(output,y)\n",
        "    data_loss = np.mean(sample_loss)\n",
        "    return data_loss\n",
        "\n",
        "class LossCategoricalCrossEntropy(Loss):\n",
        "  def forward(self,y_pred,y_real):\n",
        "    samples = len(y_pred)\n",
        "    y_pred_clipped = np.clip(y_pred,1e-7,1-1e-7)\n",
        "\n",
        "    if len(y_real.shape) == 1:\n",
        "      correct_confidence = y_pred_clipped[\n",
        "                                        range(samples),\n",
        "                                        y_real]\n",
        "    elif len(y_real.shape) == 2:\n",
        "      correct_confidence = np.sum(y_pred_clipped*y_real,\n",
        "                                  axis=1)\n",
        "\n",
        "    negative_log_likehoods = np.log(correct_confidence)\n",
        "\n",
        "    return negative_log_likehoods\n",
        "\n",
        "  def backward(self,dvalues,y_true):\n",
        "    \"\"\"\n",
        "    dvalues : valores prediccion\n",
        "    y_true : valores reales\n",
        "    \"\"\"\n",
        "    samples = len(dvalues)\n",
        "    labels = len(dvalues[0])\n",
        "\n",
        "    #One-hot encoding\n",
        "    if len(y_true.shape) == 1:\n",
        "      y_true = np.eye(labels)[y_true]\n",
        "\n",
        "    #calculando el gradiente\n",
        "    self.dinputs = -y_true/dvalues\n",
        "    #normalizar el gradiente\n",
        "    self.dinputs = self.dinputs / samples\n",
        "\n",
        "    "
      ],
      "metadata": {
        "id": "xs7aP8SjqxW2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Implementando la derivada simplificada\n",
        "class ActivationSoftmaxLossCategoricalCrossEntropy():\n",
        "\n",
        "  def __init__(self):\n",
        "    self.activation = ActivationSoftmax()\n",
        "    self.loss = LossCategoricalCrossEntropy()\n",
        "\n",
        "  def forward(self,inputs,y_true):\n",
        "    self.activation.forward(inputs)\n",
        "    self.output = self.activation.output\n",
        "    return self.loss.calculate(self.output,y_true)\n",
        "\n",
        "  def backward(self,dvalues,y_true):\n",
        "\n",
        "    samples = len(dvalues)\n",
        "\n",
        "    #quitando one-hot encoding del label real\n",
        "    if len(y_true.shape) == 2:\n",
        "      y_true = np.argmax(y_true,axis=1)#Obtengo el indice del valor maximo de cada columna\n",
        "    \n",
        "\n",
        "    self.dinputs = dvalues.copy()\n",
        "    #calculo gradiente\n",
        "    self.dinputs[range(samples), y_true] -= 1\n",
        "    #Normalizacion del gradiente\n",
        "    self.dinputs = self.dinputs / samples"
      ],
      "metadata": {
        "id": "7kKqVWJuzJYT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class OptimizerSGD():\n",
        "  def __init__(self,learning_rate=1.0,decay=0., momentum = 0.):\n",
        "    self.learning_rate = learning_rate\n",
        "    self.current_learning_rate = learning_rate\n",
        "    self.decay = decay\n",
        "    self.iterations = 0\n",
        "    self.momentum = momentum\n",
        "\n",
        "  def pre_update_params(self):\n",
        "    if self.decay:\n",
        "      self.current_learning_rate = self.learning_rate * (1.0 / (1.0 + self.decay * self.iterations))\n",
        "\n",
        "  def update_params(self,layer):\n",
        "    if self.momentum:\n",
        "      #inicializacion de la matriz de momentos\n",
        "      if not hasattr(layer, 'weight_momentums'):\n",
        "        layer.weight_momentums = np.zeros_like(layer.weights)\n",
        "        layer.bias_momentums = np.zeros_like(layer.biases)\n",
        "        \n",
        "      weight_updates = self.momentum * layer.weight_momentums - self.current_learning_rate * layer.dweights\n",
        "      layer.weight_momentums = weight_updates \n",
        "\n",
        "      bias_updates = self.momentum * layer.bias_momentums - self.current_learning_rate * layer.dbiases\n",
        "      layer.bias_momentums = bias_updates\n",
        "    \n",
        "    else:\n",
        "      weight_updates = - self.current_learning_rate * layer.dweights\n",
        "      bias_updates = - self.current_learning_rate * layer.dbiases\n",
        "\n",
        "\n",
        "    layer.weights += weight_updates\n",
        "    layer.biases += bias_updates\n",
        "  \n",
        "  def post_update_params(self):\n",
        "    self.iterations += 1\n",
        "\n",
        "class OptimizerAdaGrad():\n",
        "  def __init__(self,learning_rate=1.0,decay=0., epsilon = 1e-7):\n",
        "    self.learning_rate = learning_rate\n",
        "    self.current_learning_rate = learning_rate\n",
        "    self.decay = decay\n",
        "    self.iterations = 0\n",
        "    self.epsilon = epsilon\n",
        "\n",
        "  def pre_update_params(self):\n",
        "    if self.decay:\n",
        "      self.current_learning_rate = self.learning_rate * (1.0 / (1.0 + self.decay * self.iterations))\n",
        "\n",
        "  def update_params(self,layer):\n",
        "    #inicializacion de la matriz de momentos\n",
        "    if not hasattr(layer, 'weight_cache'):\n",
        "      layer.weight_cache = np.zeros_like(layer.weights)\n",
        "      layer.bias_cache = np.zeros_like(layer.biases)\n",
        "      \n",
        "    layer.weight_cache += layer.dweights**2\n",
        "\n",
        "    layer.bias_cache += layer.dbiases**2\n",
        "    \n",
        "\n",
        "    layer.weights += -self.current_learning_rate * layer.dweights / (np.sqrt(layer.weight_cache)+self.epsilon)\n",
        "    layer.biases += -self.current_learning_rate * layer.dbiases / (np.sqrt(layer.bias_cache)+self.epsilon)\n",
        "  \n",
        "  def post_update_params(self):\n",
        "    self.iterations += 1\n",
        "\n",
        "class OptimizerRMSprop():\n",
        "  def __init__(self,learning_rate=0.001, decay=0., epsilon=1e-7,rho=0.9):\n",
        "    self.learning_rate = learning_rate\n",
        "    self.current_learning_rate = learning_rate\n",
        "    self.decay = decay\n",
        "    self.iterations = 0\n",
        "    self.epsilon = epsilon\n",
        "    self.rho = rho\n",
        "  \n",
        "  def pre_update_params(self):\n",
        "    if self.decay:\n",
        "      self.current_learning_rate = self.current_learning_rate * (1. / (1. + self.decay * self.iterations))\n",
        "\n",
        "  def update_params(self,layer):\n",
        "    if not hasattr(layer,'weight_cache'):\n",
        "      layer.weight_cache = np.zeros_like(layer.weights)\n",
        "      layer.bias_cache = np.zeros_like(layer.biases)\n",
        "\n",
        "    layer.weight_cache = self.rho * layer.weight_cache + (1 - self.rho) * layer.dweights**2\n",
        "    layer.bias_cache = self.rho * layer.bias_cache + (1 - self.rho) * layer.dbiases**2\n",
        "\n",
        "    layer.weights += -self.current_learning_rate * layer.dweights / (np.sqrt(layer.weight_cache) + self.epsilon)\n",
        "    layer.biases += -self.current_learning_rate * layer.dbiases / (np.sqrt(layer.bias_cache) + self.epsilon)\n",
        "\n",
        "  def post_update_params(self):\n",
        "    self.iterations += 1\n",
        "\n",
        "\n",
        "class OptimizerAdam():\n",
        "  def __init__(self, learning_rate=0.001, decay=0., epsilon=1e-7, beta_1=0.9, beta_2=0.999):\n",
        "    self.learning_rate = learning_rate\n",
        "    self.current_learning_rate = learning_rate\n",
        "    self.decay = decay\n",
        "    self.iterations = 0\n",
        "    self.epsilon = epsilon\n",
        "    self.beta_1 = beta_1\n",
        "    self.beta_2 = beta_2\n",
        "  \n",
        "  def pre_update_params(self):\n",
        "    if self.decay:\n",
        "      self.current_learning_rate = self.current_learning_rate * (1. / (1. + self.decay * self.iterations))\n",
        "\n",
        "  def update_params(self,layer):\n",
        "    if not hasattr(layer,'weight_cache'):\n",
        "      layer.weight_momentums = np.zeros_like(layer.weights)\n",
        "      layer.weight_cache = np.zeros_like(layer.weights)\n",
        "\n",
        "      layer.bias_momentums = np.zeros_like(layer.biases)\n",
        "      layer.bias_cache = np.zeros_like(layer.biases)\n",
        "\n",
        "    layer.weight_momentums = self.beta_1 * layer.weight_momentums + (1 - self.beta_1) * layer.dweights\n",
        "    layer.bias_momentums = self.beta_1 * layer.bias_momentums + (1 - self.beta_1) * layer.dbiases\n",
        "    \n",
        "    weight_momentums_corrected = layer.weight_momentums / (1 - self.beta_1 ** (self.iterations + 1))\n",
        "    bias_momentums_corrected = layer.bias_momentums / (1 - self.beta_1 ** (self.iterations + 1))\n",
        "\n",
        "    layer.weight_cache = self.beta_2 * layer.weight_cache + (1 - self.beta_2) * layer.dweights**2\n",
        "    layer.bias_cache = self.beta_2 * layer.bias_cache + (1 - self.beta_2) * layer.dbiases**2\n",
        "\n",
        "    weight_cache_corrected = layer.weight_cache / (1 - self.beta_2 ** (self.iterations + 1))\n",
        "    bias_cache_corrected = layer.bias_cache / (1 - self.beta_2 ** (self.iterations + 1))\n",
        "\n",
        "    layer.weights += -self.current_learning_rate * weight_momentums_corrected / (np.sqrt(weight_cache_corrected) + self.epsilon)\n",
        "    layer.biases += -self.current_learning_rate * bias_momentums_corrected / (np.sqrt(bias_cache_corrected) + self.epsilon)\n",
        "\n",
        "  def post_update_params(self):\n",
        "    self.iterations += 1\n",
        "\n"
      ],
      "metadata": {
        "id": "Y0CYNotl-_Bj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "softmax_output = np.array([[0.7,0.1,0.2],\n",
        "                           [0.1,0.5,0.4],\n",
        "                           [0.02,0.9,0.08]])\n",
        "\n",
        "class_targets = np.array([0, 1, 1])\n",
        "\n",
        "#Esta implementacion resulta ser aproximadamente 7 veces mas rapida\n",
        "softmax_loss = ActivationSoftmaxLossCategoricalCrossEntropy()\n",
        "softmax_loss.backward(softmax_output, class_targets)\n",
        "dvalues1 = softmax_loss.dinputs\n",
        "\n",
        "\n",
        "activation = ActivationSoftmax()\n",
        "activation.output = softmax_output\n",
        "loss = LossCategoricalCrossEntropy()\n",
        "loss.backward(softmax_output,class_targets)\n",
        "activation.backward(loss.dinputs)\n",
        "dvalues2 = activation.dinputs\n",
        "\n",
        "print(\"Gradients: combined loss and activation:\")\n",
        "print(dvalues1)\n",
        "print(\"Gradients: separate loss and activation:\")\n",
        "print(dvalues2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yruSztEa3aMy",
        "outputId": "6fca94b5-3906-49b8-9e34-0eb565e1ae7e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradients: combined loss and activation:\n",
            "[[-0.1         0.03333333  0.06666667]\n",
            " [ 0.03333333 -0.16666667  0.13333333]\n",
            " [ 0.00666667 -0.03333333  0.02666667]]\n",
            "Gradients: separate loss and activation:\n",
            "[[-0.09999999  0.03333334  0.06666667]\n",
            " [ 0.03333334 -0.16666667  0.13333334]\n",
            " [ 0.00666667 -0.03333333  0.02666667]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X,y = spiral_data(samples=100, classes=3)\n",
        "dense1 = LayerDense(2,64)\n",
        "activation1 = ActivationReLU()\n",
        "dense2 = LayerDense(64,3)\n",
        "loss_activation = ActivationSoftmaxLossCategoricalCrossEntropy()\n",
        "# activation2 = ActivationSoftmax()\n",
        "# loss_function = LossCategoricalCrossEntropy()\n",
        "#optimizer = OptimizerSGD(decay=1e-3)\n",
        "#optimizer = OptimizerAdaGrad(decay=1e-4)\n",
        "#optimizer = OptimizerRMSprop(learning_rate=0.02, decay=1e-4, rho=0.999) \n",
        "optimizer = OptimizerAdam(learning_rate=0.05, decay=5e-7)\n",
        "\n",
        "for epoch in range(10001):\n",
        "  dense1.forward(X)\n",
        "  activation1.forward(dense1.output)\n",
        "\n",
        "  dense2.forward(activation1.output)\n",
        "  #activation2.forward(dense2.output)\n",
        "\n",
        "  #loss = loss_function.calculate(activation2.output,y)\n",
        "\n",
        "  loss = loss_activation.forward(dense2.output,y)\n",
        "\n",
        "  predictions = np.argmax(loss_activation.output, axis=1)\n",
        "  if len(y.shape) == 2:\n",
        "    y = np.argmax(y, axis=1)\n",
        "  accuracy =  np.mean(predictions==y)\n",
        "\n",
        "  if not epoch % 100:\n",
        "    print(f'epoch: {epoch}, acc: {accuracy:.3f}, loss: {loss:.3f}, learning rate {optimizer.current_learning_rate}')\n",
        "\n",
        "\n",
        "  #Backward pass\n",
        "  loss_activation.backward(loss_activation.output,y)\n",
        "  dense2.backward(loss_activation.dinputs)\n",
        "  activation1.backward(dense2.dinputs)\n",
        "  dense1.backward(activation1.dinputs)\n",
        "\n",
        "  optimizer.pre_update_params()\n",
        "  optimizer.update_params(dense1)\n",
        "  optimizer.update_params(dense2)\n",
        "  optimizer.post_update_params()\n",
        "\n",
        "print(dense1.dweights)\n",
        "print(dense1.dbiases)\n",
        "print(dense2.dweights)\n",
        "print(dense2.dbiases)"
      ],
      "metadata": {
        "id": "g1ruU6cihwUk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a0837bc-4883-47b9-8f34-499eba726bf6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch: 0, acc: 0.350, loss: -1.099, learning rate 0.05\n",
            "epoch: 100, acc: 0.570, loss: -0.902, learning rate 0.049876405061425835\n",
            "epoch: 200, acc: 0.620, loss: -0.864, learning rate 0.0495049832512073\n",
            "epoch: 300, acc: 0.640, loss: -0.821, learning rate 0.04889128328816377\n",
            "epoch: 400, acc: 0.637, loss: -0.794, learning rate 0.0480444037704678\n",
            "epoch: 500, acc: 0.643, loss: -0.780, learning rate 0.04697676874534755\n",
            "epoch: 600, acc: 0.667, loss: -0.759, learning rate 0.045703824504893936\n",
            "epoch: 700, acc: 0.660, loss: -0.743, learning rate 0.0442436679478586\n",
            "epoch: 800, acc: 0.670, loss: -0.720, learning rate 0.042616618997994474\n",
            "epoch: 900, acc: 0.687, loss: -0.693, learning rate 0.04084475118162425\n",
            "epoch: 1000, acc: 0.750, loss: -0.630, learning rate 0.03895139548299276\n",
            "epoch: 1100, acc: 0.780, loss: -0.602, learning rate 0.036960632988575076\n",
            "epoch: 1200, acc: 0.780, loss: -0.572, learning rate 0.03489679160511355\n",
            "epoch: 1300, acc: 0.790, loss: -0.547, learning rate 0.03278396132495571\n",
            "epoch: 1400, acc: 0.777, loss: -0.534, learning rate 0.030645541177404013\n",
            "epoch: 1500, acc: 0.790, loss: -0.526, learning rate 0.02850382922992508\n",
            "epoch: 1600, acc: 0.790, loss: -0.518, learning rate 0.02637966488881907\n",
            "epoch: 1700, acc: 0.787, loss: -0.511, learning rate 0.02429213040665953\n",
            "epoch: 1800, acc: 0.793, loss: -0.506, learning rate 0.02225831604903483\n",
            "epoch: 1900, acc: 0.793, loss: -0.501, learning rate 0.02029315091926959\n",
            "epoch: 2000, acc: 0.827, loss: -0.472, learning rate 0.01840929909232032\n",
            "epoch: 2100, acc: 0.840, loss: -0.436, learning rate 0.016617118560453482\n",
            "epoch: 2200, acc: 0.850, loss: -0.407, learning rate 0.014924678619373358\n",
            "epoch: 2300, acc: 0.853, loss: -0.387, learning rate 0.013337829780562247\n",
            "epoch: 2400, acc: 0.863, loss: -0.374, learning rate 0.011860319119473552\n",
            "epoch: 2500, acc: 0.863, loss: -0.366, learning rate 0.010493943175001245\n",
            "epoch: 2600, acc: 0.863, loss: -0.359, learning rate 0.00923873009900314\n",
            "epoch: 2700, acc: 0.863, loss: -0.354, learning rate 0.008093142693950782\n",
            "epoch: 2800, acc: 0.867, loss: -0.350, learning rate 0.007054294236001027\n",
            "epoch: 2900, acc: 0.867, loss: -0.347, learning rate 0.00611816951294818\n",
            "epoch: 3000, acc: 0.870, loss: -0.344, learning rate 0.005279844257307346\n",
            "epoch: 3100, acc: 0.867, loss: -0.342, learning rate 0.004533697066202347\n",
            "epoch: 3200, acc: 0.867, loss: -0.340, learning rate 0.0038736089135087077\n",
            "epoch: 3300, acc: 0.867, loss: -0.338, learning rate 0.003293146420284862\n",
            "epoch: 3400, acc: 0.867, loss: -0.336, learning rate 0.0027857261065572284\n",
            "epoch: 3500, acc: 0.867, loss: -0.334, learning rate 0.0023447578576989737\n",
            "epoch: 3600, acc: 0.860, loss: -0.333, learning rate 0.0019637667669094236\n",
            "epoch: 3700, acc: 0.863, loss: -0.332, learning rate 0.0016364933354569265\n",
            "epoch: 3800, acc: 0.863, loss: -0.331, learning rate 0.0013569727070496412\n",
            "epoch: 3900, acc: 0.863, loss: -0.331, learning rate 0.001119594172990414\n",
            "epoch: 4000, acc: 0.863, loss: -0.330, learning rate 0.000919142609228804\n",
            "epoch: 4100, acc: 0.863, loss: -0.330, learning rate 0.0007508238000162101\n",
            "epoch: 4200, acc: 0.863, loss: -0.329, learning rate 0.0006102757756699268\n",
            "epoch: 4300, acc: 0.867, loss: -0.329, learning rate 0.0004935683577695847\n",
            "epoch: 4400, acc: 0.867, loss: -0.328, learning rate 0.00039719308016708716\n",
            "epoch: 4500, acc: 0.867, loss: -0.328, learning rate 0.00031804555589382813\n",
            "epoch: 4600, acc: 0.867, loss: -0.328, learning rate 0.00025340220591227173\n",
            "epoch: 4700, acc: 0.867, loss: -0.328, learning rate 0.00020089307242635634\n",
            "epoch: 4800, acc: 0.867, loss: -0.328, learning rate 0.00015847222243136548\n",
            "epoch: 4900, acc: 0.867, loss: -0.328, learning rate 0.0001243870197247375\n",
            "epoch: 5000, acc: 0.867, loss: -0.327, learning rate 9.71473168955103e-05\n",
            "epoch: 5100, acc: 0.867, loss: -0.327, learning rate 7.549540174261466e-05\n",
            "epoch: 5200, acc: 0.867, loss: -0.327, learning rate 5.837733176085825e-05\n",
            "epoch: 5300, acc: 0.867, loss: -0.327, learning rate 4.491611028726038e-05\n",
            "epoch: 5400, acc: 0.867, loss: -0.327, learning rate 3.438700124796313e-05\n",
            "epoch: 5500, acc: 0.867, loss: -0.327, learning rate 2.6195147215341062e-05\n",
            "epoch: 5600, acc: 0.867, loss: -0.327, learning rate 1.9855547406706964e-05\n",
            "epoch: 5700, acc: 0.867, loss: -0.327, learning rate 1.4975367066981509e-05\n",
            "epoch: 5800, acc: 0.867, loss: -0.327, learning rate 1.1238485408004185e-05\n",
            "epoch: 5900, acc: 0.867, loss: -0.327, learning rate 8.392143508312201e-06\n",
            "epoch: 6000, acc: 0.867, loss: -0.327, learning rate 6.235523663129205e-06\n",
            "epoch: 6100, acc: 0.867, loss: -0.327, learning rate 4.610074921762313e-06\n",
            "epoch: 6200, acc: 0.867, loss: -0.327, learning rate 3.391393357571797e-06\n",
            "epoch: 6300, acc: 0.867, loss: -0.327, learning rate 2.4824675757585622e-06\n",
            "epoch: 6400, acc: 0.867, loss: -0.327, learning rate 1.808107926184436e-06\n",
            "epoch: 6500, acc: 0.867, loss: -0.327, learning rate 1.3103899959519274e-06\n",
            "epoch: 6600, acc: 0.867, loss: -0.327, learning rate 9.449576585319987e-07\n",
            "epoch: 6700, acc: 0.867, loss: -0.327, learning rate 6.780470000665035e-07\n",
            "epoch: 6800, acc: 0.867, loss: -0.327, learning rate 4.841088540382425e-07\n",
            "epoch: 6900, acc: 0.867, loss: -0.327, learning rate 3.439237262999786e-07\n",
            "epoch: 7000, acc: 0.867, loss: -0.327, learning rate 2.4311807162988357e-07\n",
            "epoch: 7100, acc: 0.867, loss: -0.327, learning rate 1.7100485782506467e-07\n",
            "epoch: 7200, acc: 0.867, loss: -0.327, learning rate 1.196839355969608e-07\n",
            "epoch: 7300, acc: 0.867, loss: -0.327, learning rate 8.334884616029957e-08\n",
            "epoch: 7400, acc: 0.867, loss: -0.327, learning rate 5.77563509176212e-08\n",
            "epoch: 7500, acc: 0.867, loss: -0.327, learning rate 3.9823225472575604e-08\n",
            "epoch: 7600, acc: 0.867, loss: -0.327, learning rate 2.73218283538773e-08\n",
            "epoch: 7700, acc: 0.867, loss: -0.327, learning rate 1.8651760573603e-08\n",
            "epoch: 7800, acc: 0.867, loss: -0.327, learning rate 1.2669712511068805e-08\n",
            "epoch: 7900, acc: 0.867, loss: -0.327, learning rate 8.563487177483035e-09\n",
            "epoch: 8000, acc: 0.867, loss: -0.327, learning rate 5.7593253608031186e-09\n",
            "epoch: 8100, acc: 0.867, loss: -0.327, learning rate 3.8541601266814095e-09\n",
            "epoch: 8200, acc: 0.867, loss: -0.327, learning rate 2.5664049275301665e-09\n",
            "epoch: 8300, acc: 0.867, loss: -0.327, learning rate 1.7004270325792175e-09\n",
            "epoch: 8400, acc: 0.867, loss: -0.327, learning rate 1.1210586191968892e-09\n",
            "epoch: 8500, acc: 0.867, loss: -0.327, learning rate 7.354213821351292e-10\n",
            "epoch: 8600, acc: 0.867, loss: -0.327, learning rate 4.800449441510118e-10\n",
            "epoch: 8700, acc: 0.867, loss: -0.327, learning rate 3.117922900459937e-10\n",
            "epoch: 8800, acc: 0.867, loss: -0.327, learning rate 2.0150544020166838e-10\n",
            "epoch: 8900, acc: 0.867, loss: -0.327, learning rate 1.2958245962906908e-10\n",
            "epoch: 9000, acc: 0.867, loss: -0.327, learning rate 8.291704377694996e-11\n",
            "epoch: 9100, acc: 0.867, loss: -0.327, learning rate 5.279340282557151e-11\n",
            "epoch: 9200, acc: 0.867, loss: -0.327, learning rate 3.3446745358215424e-11\n",
            "epoch: 9300, acc: 0.867, loss: -0.327, learning rate 2.108465707823855e-11\n",
            "epoch: 9400, acc: 0.867, loss: -0.327, learning rate 1.3225673524701465e-11\n",
            "epoch: 9500, acc: 0.867, loss: -0.327, learning rate 8.254822497522298e-12\n",
            "epoch: 9600, acc: 0.867, loss: -0.327, learning rate 5.126683102383918e-12\n",
            "epoch: 9700, acc: 0.867, loss: -0.327, learning rate 3.168138121788358e-12\n",
            "epoch: 9800, acc: 0.867, loss: -0.327, learning rate 1.948097771143998e-12\n",
            "epoch: 9900, acc: 0.867, loss: -0.327, learning rate 1.1919457981008212e-12\n",
            "epoch: 10000, acc: 0.867, loss: -0.327, learning rate 7.256738734542257e-13\n",
            "[[ 0.00000000e+00  1.35526899e-03  0.00000000e+00  1.67801889e-04\n",
            "  -2.62095946e-05  0.00000000e+00  0.00000000e+00  4.27100400e-04\n",
            "   4.45746467e-04 -6.81668753e-04  2.29625977e-04 -4.00728866e-04\n",
            "   0.00000000e+00 -1.07076031e-03  0.00000000e+00  0.00000000e+00\n",
            "   2.10370097e-04 -1.06003587e-04 -5.39263012e-04  2.23440374e-03\n",
            "  -1.02490734e-03  0.00000000e+00 -1.17963017e-03  1.32402207e-03\n",
            "  -1.32875750e-04  0.00000000e+00  1.35252532e-03 -9.36858240e-04\n",
            "   2.49154679e-03  0.00000000e+00  0.00000000e+00  2.12682877e-04\n",
            "  -4.39120689e-04 -2.49475823e-04  0.00000000e+00  0.00000000e+00\n",
            "   4.62698139e-04  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00 -1.11708068e-03  0.00000000e+00  0.00000000e+00\n",
            "  -4.50647611e-04  0.00000000e+00  0.00000000e+00 -2.73911137e-04\n",
            "  -5.98697443e-05  4.26322258e-05  0.00000000e+00  2.78654072e-04\n",
            "  -1.33808629e-04  0.00000000e+00  1.29216351e-03 -1.41524256e-03\n",
            "   1.68332132e-04  0.00000000e+00  2.27923767e-04  1.04283122e-03\n",
            "   0.00000000e+00 -1.55250105e-04  0.00000000e+00 -9.77656571e-04]\n",
            " [ 0.00000000e+00  1.16048337e-04  0.00000000e+00  1.55305843e-05\n",
            "   5.96064092e-05  0.00000000e+00  0.00000000e+00  5.52588353e-05\n",
            "  -4.69001789e-05 -4.74853259e-05  7.23156263e-05 -4.48215142e-04\n",
            "   0.00000000e+00 -6.01351749e-05  0.00000000e+00  0.00000000e+00\n",
            "   5.26587137e-05 -3.07029040e-05  1.51378204e-04 -2.29304933e-04\n",
            "  -1.00157638e-04  0.00000000e+00  2.81658140e-04  1.14365364e-04\n",
            "   6.76833850e-04  0.00000000e+00  1.15044320e-04 -1.70750704e-04\n",
            "  -8.53846315e-04  0.00000000e+00  0.00000000e+00  5.72786666e-05\n",
            "   3.29834111e-05 -8.92673706e-05  0.00000000e+00  0.00000000e+00\n",
            "   3.75053896e-05  0.00000000e+00  0.00000000e+00  0.00000000e+00\n",
            "   0.00000000e+00 -2.08264682e-04  0.00000000e+00  0.00000000e+00\n",
            "  -2.33184299e-04  0.00000000e+00  0.00000000e+00 -1.03442417e-03\n",
            "   1.44987935e-05  4.49154468e-05  0.00000000e+00  1.28903391e-03\n",
            "   8.00363996e-05  0.00000000e+00  1.11888578e-04  2.66844174e-04\n",
            "   1.44656660e-05  0.00000000e+00  6.97218857e-05 -4.45324782e-04\n",
            "   0.00000000e+00  7.75726279e-04  0.00000000e+00 -6.25146204e-05]]\n",
            "[[ 0.          0.00077848  0.         -0.00032901  0.00043908  0.\n",
            "   0.         -0.0002703  -0.00114066 -0.00140017 -0.00030079  0.00113156\n",
            "   0.          0.00098643  0.          0.         -0.00031536  0.0001467\n",
            "   0.00084004 -0.00182943 -0.0010406   0.          0.00162208  0.00075419\n",
            "  -0.00075336  0.          0.00078149 -0.0014497   0.00209708  0.\n",
            "   0.         -0.00030699  0.00020879 -0.00063204  0.          0.\n",
            "  -0.00163732  0.          0.          0.          0.         -0.00173871\n",
            "   0.          0.          0.00012713  0.          0.         -0.00155273\n",
            "   0.00017596  0.00012232  0.          0.00578975 -0.00028476  0.\n",
            "   0.00073451  0.00182642 -0.00033326  0.         -0.00030461  0.00068774\n",
            "   0.         -0.00045714  0.          0.00089582]]\n",
            "[[ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [-2.76107137e-04  2.62016663e-04  1.40960328e-05]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [-1.08164953e-04  3.04132962e-04 -1.95964327e-04]\n",
            " [-1.82232732e-04 -2.07713209e-04  3.89951630e-04]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 1.28394473e-04 -3.90436529e-04  2.62035639e-04]\n",
            " [ 4.80742339e-04 -2.93931284e-04 -1.86813981e-04]\n",
            " [ 7.77965586e-04 -1.26188644e-03  4.83968150e-04]\n",
            " [-9.52828559e-05  3.21310537e-04 -2.26024014e-04]\n",
            " [-3.89309629e-04  3.54616321e-04  3.47097448e-05]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 1.06662032e-04  2.10575599e-04 -3.17231403e-04]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [-9.91579436e-05  3.16594465e-04 -2.17432855e-04]\n",
            " [-1.22794328e-04 -3.81905738e-06  1.26620289e-04]\n",
            " [-3.30042785e-05  1.24613682e-04 -9.16024001e-05]\n",
            " [ 8.37766549e-07 -5.19666828e-05  5.11288781e-05]\n",
            " [-2.21667665e-06  4.42405189e-06 -2.20595939e-06]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 7.87890676e-05  1.91747618e-04 -2.70529941e-04]\n",
            " [-2.71210010e-04  2.58130342e-04  1.30850976e-05]\n",
            " [ 1.97175090e-04 -1.76877584e-04 -2.03000091e-05]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [-2.74617487e-04  2.60056375e-04  1.45666227e-05]\n",
            " [-2.49720440e-04  2.37949876e-04  1.17755453e-05]\n",
            " [-9.31003480e-04  5.41385321e-04  3.89617053e-04]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [-9.87873864e-05  3.16892605e-04 -2.18101544e-04]\n",
            " [-2.06090626e-04  1.90889055e-04  1.52044295e-05]\n",
            " [-9.57135984e-04  8.51273537e-04  1.05910847e-04]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 5.15947686e-05 -1.10803195e-03  1.05644332e-03]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [-2.82103807e-04  2.66718504e-04  1.53909550e-05]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [-3.58392543e-04  3.64516687e-04 -6.10784491e-06]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 1.48415635e-03 -1.40082370e-03 -8.33055165e-05]\n",
            " [-1.24025857e-04 -1.00008970e-06  1.25033112e-04]\n",
            " [-6.61711092e-05  7.68222162e-05 -1.06441585e-05]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [-1.14538656e-04  1.10721230e-04  3.81677592e-06]\n",
            " [-3.28997271e-06  2.96117651e-04 -2.92829442e-04]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [-2.68426549e-04  2.55792373e-04  1.26395225e-05]\n",
            " [ 1.37234412e-04  2.41330286e-04 -3.78558325e-04]\n",
            " [-1.08623222e-04  3.07073497e-04 -1.98446578e-04]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [-9.76336887e-05  3.22413514e-04 -2.24776129e-04]\n",
            " [ 1.09155313e-04  2.87537114e-04 -3.96687275e-04]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [-1.23905513e-04  7.88816251e-05  4.50234875e-05]\n",
            " [ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
            " [ 1.07543339e-04  2.06230485e-04 -3.13767290e-04]]\n",
            "[[-1.1997204e-04  7.9277648e-05  4.0720508e-05]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "YPWkBaDpVMRa"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}